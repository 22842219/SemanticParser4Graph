{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.9.2 in /home/22842219/.local/lib/python3.8/site-packages (4.9.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.2) (21.3)\n",
      "Requirement already satisfied: sacremoses in /home/22842219/.local/lib/python3.8/site-packages (from transformers==4.9.2) (0.0.53)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/22842219/.local/lib/python3.8/site-packages (from transformers==4.9.2) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.2) (4.64.1)\n",
      "Collecting huggingface-hub==0.0.12\n",
      "  Using cached huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.2) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.9.2) (5.3.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.9.2) (2.22.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers==4.9.2) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.2) (1.23.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.9.2) (3.0.9)\n",
      "Requirement already satisfied: six in /home/22842219/.local/lib/python3.8/site-packages (from sacremoses->transformers==4.9.2) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from sacremoses->transformers==4.9.2) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.9.2) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.0.12->transformers==4.9.2) (4.4.0)\n",
      "\u001b[31mERROR: datasets 1.14.0 has requirement huggingface-hub<0.1.0,>=0.0.19, but you'll have huggingface-hub 0.0.12 which is incompatible.\u001b[0m\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.19\n",
      "    Uninstalling huggingface-hub-0.0.19:\n",
      "      Successfully uninstalled huggingface-hub-0.0.19\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/22842219/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed huggingface-hub-0.0.12\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: datasets==1.14.0 in /home/22842219/.local/lib/python3.8/site-packages (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets==1.14.0) (1.23.4)\n",
      "Collecting huggingface-hub<0.1.0,>=0.0.19\n",
      "  Using cached huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: xxhash in /home/22842219/.local/lib/python3.8/site-packages (from datasets==1.14.0) (3.2.0)\n",
      "Requirement already satisfied: pandas in /home/22842219/.local/lib/python3.8/site-packages (from datasets==1.14.0) (1.5.3)\n",
      "Requirement already satisfied: dill in /home/22842219/.local/lib/python3.8/site-packages (from datasets==1.14.0) (0.3.6)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /home/22842219/.local/lib/python3.8/site-packages (from datasets==1.14.0) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /home/22842219/.local/lib/python3.8/site-packages (from datasets==1.14.0) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in /home/22842219/.local/lib/python3.8/site-packages (from datasets==1.14.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/22842219/.local/lib/python3.8/site-packages (from datasets==1.14.0) (2023.1.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets==1.14.0) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets==1.14.0) (2.22.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==1.14.0) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets==1.14.0) (3.0.12)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets==1.14.0) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets==1.14.0) (4.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/22842219/.local/lib/python3.8/site-packages (from pandas->datasets==1.14.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/22842219/.local/lib/python3.8/site-packages (from pandas->datasets==1.14.0) (2022.7.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/22842219/.local/lib/python3.8/site-packages (from aiohttp->datasets==1.14.0) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/22842219/.local/lib/python3.8/site-packages (from aiohttp->datasets==1.14.0) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/22842219/.local/lib/python3.8/site-packages (from aiohttp->datasets==1.14.0) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/22842219/.local/lib/python3.8/site-packages (from aiohttp->datasets==1.14.0) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/22842219/.local/lib/python3.8/site-packages (from aiohttp->datasets==1.14.0) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/22842219/.local/lib/python3.8/site-packages (from aiohttp->datasets==1.14.0) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/22842219/.local/lib/python3.8/site-packages (from aiohttp->datasets==1.14.0) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets==1.14.0) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/22842219/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets==1.14.0) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets==1.14.0) (2.8)\n",
      "\u001b[31mERROR: transformers 4.9.2 has requirement huggingface-hub==0.0.12, but you'll have huggingface-hub 0.0.19 which is incompatible.\u001b[0m\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.12\n",
      "    Uninstalling huggingface-hub-0.0.12:\n",
      "      Successfully uninstalled huggingface-hub-0.0.12\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/22842219/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed huggingface-hub-0.0.19\n",
      "Requirement already satisfied: sentencepiece in /home/22842219/.local/lib/python3.8/site-packages (0.1.97)\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.8.0+cu111 in /home/22842219/.local/lib/python3.8/site-packages (1.8.0+cu111)\n",
      "Requirement already satisfied: torchvision==0.9.0+cu111 in /home/22842219/.local/lib/python3.8/site-packages (0.9.0+cu111)\n",
      "Requirement already satisfied: torchaudio==0.8.0 in /home/22842219/.local/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (4.4.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/lib/python3/dist-packages (from torchvision==0.9.0+cu111) (7.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.9.2\n",
    "!pip install nltk\n",
    "!pip install datasets==1.14.0\n",
    "!pip install sentencepiece\n",
    "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/UnifiedSKGG-CYPHER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140003947357232 acquired on .lock\n",
      "INFO:filelock:Lock 140003947357232 released on .lock\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from utils.configue import Configure\n",
    "from utils.training_arguments import WrappedSeq2SeqTrainingArguments\n",
    "\n",
    "from filelock import FileLock\n",
    "import nltk\n",
    "with FileLock(\".lock\") as lock:\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "    nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(txt, model, tokenizer):\n",
    "  print(\"=====❓Request=====\")\n",
    "  print(txt)\n",
    "  tokenized_txt = tokenizer([txt], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "  pred = tokenizer.batch_decode(\n",
    "      model.generate(\n",
    "        torch.LongTensor(tokenized_txt.data['input_ids']),\n",
    "        torch.LongTensor(tokenized_txt.data['attention_mask']),\n",
    "        num_beams=1, \n",
    "        max_length=256\n",
    "        ), \n",
    "      skip_special_tokens=True \n",
    "  ) # More details see utils/dataset.py and utils/trainer.py\n",
    "  print(\"=====💡Answer=====\")\n",
    "  print(pred)\n",
    "  return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "#  Spider (with cell value)\n",
    "\n",
    "# Set args here for runnning on notebook, we make them out here to make it more illustrative.\n",
    "sys.argv = ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', # This is the name of your .py launcher when you run this line of code.\n",
    "            # belows are the parameters we set, take spider for example\n",
    "            '--cfg', 'Salesforce/T5_base_prefix_spider_with_cell_value.cfg', \n",
    "            '--output_dir', './tmp']\n",
    "parser = HfArgumentParser((WrappedSeq2SeqTrainingArguments,))\n",
    "training_args, = parser.parse_args_into_dataclasses()\n",
    "set_seed(training_args.seed)\n",
    "args = Configure.Get(training_args.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hkunlp/from_all_T5_base_prefix_spider_with_cell_value2/resolve/main/spiece.model from cache at /home/22842219/.cache/huggingface/transformers/c137efa00ca821483894a38692ad1c725ea36d72bbd5b659538cdd041695674c.d6f0605ae3d57070be74b4c12206072ab332922acff822e6b5458691dbda7551\n",
      "loading file https://huggingface.co/hkunlp/from_all_T5_base_prefix_spider_with_cell_value2/resolve/main/added_tokens.json from cache at /home/22842219/.cache/huggingface/transformers/416d14f1fa4c64761400b6f1fd0ce39bcc8a2e0583e22219dc154f3c5c74b7aa.ffb6426fb410c59e57e5a4c85d9a0c58498b73237acd2662871aa605056dcc5a\n",
      "loading file https://huggingface.co/hkunlp/from_all_T5_base_prefix_spider_with_cell_value2/resolve/main/special_tokens_map.json from cache at /home/22842219/.cache/huggingface/transformers/15f4ad6c9cbcaa98da6ffb8009488d50a7d655f5135340fd7eef0d58d914c8a9.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
      "loading file https://huggingface.co/hkunlp/from_all_T5_base_prefix_spider_with_cell_value2/resolve/main/tokenizer_config.json from cache at /home/22842219/.cache/huggingface/transformers/1bdc5c54c919b1f571039d9ed008345bdc106e978e0ed2c42c419176292dbf18.0a18dfcee66f41e68b96b3066a2bf9081b22234da2236b49d2a359636c0e716b\n",
      "loading file https://huggingface.co/hkunlp/from_all_T5_base_prefix_spider_with_cell_value2/resolve/main/tokenizer.json from cache at None\n",
      "Adding  < to the vocabulary\n",
      "Adding  <= to the vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix-tuning sequence length is 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/22842219/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.a085189636e38420df5e7bdf08ad1b86f1fe33c010079ca7f15437ff95f4fe2b\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /home/22842219/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /home/22842219/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/22842219/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.a085189636e38420df5e7bdf08ad1b86f1fe33c010079ca7f15437ff95f4fe2b\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/22842219/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.a085189636e38420df5e7bdf08ad1b86f1fe33c010079ca7f15437ff95f4fe2b\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /home/22842219/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Adding  < to the vocabulary\n",
      "Adding  <= to the vocabulary\n",
      "INFO:models.unified.base:loading weights file https://huggingface.co/hkunlp/from_all_T5_base_prefix_spider_with_cell_value2/resolve/main/pytorch_model.bin from cache at /home/22842219/.cache/huggingface/transformers/41f327fe29e17e5d9d014140d45943881e9fa1e1e0d6edbac0f2e3349fa230f2.7d00fa36eca679a6d557fef946d7dd0ae3979aaa3b909db35ff6086996777b69\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model(21->1 multitasked prefix)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hkunlp/from_all_T5_base_prefix_spider_with_cell_value2\", use_fast=False)\n",
    "from models.unified.prefixtuning import Model\n",
    "model = Model(args)\n",
    "model.load(\"hkunlp/from_all_T5_base_prefix_spider_with_cell_value2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_in = \"| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country ( France ) , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id\"\n",
    "text_in = \"what is the minimum, average, and maximum age of all singers from France?\"\n",
    "# seq_out = \"SELECT avg(age) ,  min(age) ,  max(age) FROM singer WHERE country  =  'France'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====❓Request=====\n",
      "what is the minimum, average, and maximum age of all singers from France?; structed knowledge: | concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country ( France ) , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id\n",
      "=====💡Answer=====\n",
      "['select min(age), avg(age), max(age) from singer where country = \"France\"']\n"
     ]
    }
   ],
   "source": [
    "generated_sql = play(\"{}; structed knowledge: {}\".format(text_in, struct_in), model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_txt: {'input_ids': [[125, 19, 8, 2559, 6, 1348, 6, 11, 2411, 1246, 13, 66, 7634, 7, 45, 1410, 58, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "tokenized_txt: 1024\n",
      "tokenized_txt: 1024\n"
     ]
    }
   ],
   "source": [
    "tokenized_txt = tokenizer([text_in], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "print(\"tokenized_txt:\", tokenized_txt)\n",
    "print(\"tokenized_txt:\", len(tokenized_txt['input_ids'][0]))\n",
    "print(\"tokenized_txt:\", len(tokenized_txt['attention_mask'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select min(age), avg(age), max(age) from singer where country = \"France\"']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Ziyu Zhao\n",
    "Affiliation: UWA NLT-TLP GROUP\n",
    "'''\n",
    "\n",
    "import os, re, math\n",
    "from typing import Set\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "class DBengine:\n",
    "    \n",
    "    \"\"\"\n",
    "    An Entity, which takes a sqlite3 database schema.\n",
    "    Attributes:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fdb):\n",
    "\n",
    "        self.conn = sqlite3.connect(fdb)      \n",
    "        self.conn.text_factory = lambda b: b.decode(errors = 'ignore')\n",
    "\n",
    "    \n",
    "    def execute(self, query):\n",
    "      cursor = self.conn.cursor()  \n",
    "      return cursor.execute(query)  \n",
    "    \n",
    "    def get_table_names(self):\n",
    "        cursor = self.conn.cursor()  \n",
    "        return cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")     \n",
    "    \n",
    "    def get_table_values(self, table_name):\n",
    "        cursor = self.conn.cursor()  \n",
    "        return cursor.execute('SELECT * from {}'.format(table_name))  \n",
    "\n",
    "    def get_schema(self, table_name):\n",
    "        cursor = self.conn.cursor()  \n",
    "        cursor.execute('SELECT sql FROM sqlite_master WHERE type = \"table\" and name= \"{}\";'.format(table_name))  \n",
    "        result = cursor.fetchone()\n",
    "        if result is None:\n",
    "            raise ValueError(\"Table %s does not exist\" % table_name)\n",
    "        return result[0].strip()  \n",
    "\n",
    "    def get_outbound_foreign_keys(self, table_name):\n",
    "        cursor = self.conn.cursor() \n",
    "        infos = cursor.execute(\"PRAGMA foreign_key_list([{}])\".format(table_name)).fetchall()\n",
    "        table_constraints = []\n",
    "        pks_fks_dict ={}\n",
    "        for info in infos:\n",
    "            if info is not None:\n",
    "                id, seq, ref_table, from_, to_, on_update, on_delete, match = info\n",
    "                table_constraints.append(\n",
    "                {\"this_table\": table_name, \"column\": from_, \"ref_table\": ref_table, \"ref_column\": to_}\n",
    "                )\n",
    "                pks_fks_dict[from_]= to_  # ref_column (aka pk in ref_table) is supposed to be the same with column. \n",
    "        return table_constraints, pks_fks_dict\n",
    "\n",
    "    def get_primay_keys(self, table_name):\n",
    "        cursor = self.conn.cursor() \n",
    "        pks = cursor.execute('SELECT l.name FROM pragma_table_info(\"{}\") as l WHERE l.pk <> 0;'.format(table_name)).fetchall()\n",
    "        return pks  \n",
    "    \n",
    "    def check_compound_pk(self, primary_keys):\n",
    "        compound_pk_check=False\n",
    "        if len(primary_keys)>1: \n",
    "            compound_pk_check=True\n",
    "        return compound_pk_check\n",
    "\n",
    "    def close(self):\n",
    "        self.conn.cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select min(age), avg(age), max(age) from singer where country = \"France\"\n",
      "sql_ans: [(25, 34.5, 43)]\n"
     ]
    }
   ],
   "source": [
    "db_path = '/home/22842219/Desktop/openSource/UnifiedSKGG-Cypher/data/downloads/spider/database/concert_singer/concert_singer.sqlite'\n",
    "engine = DBengine(db_path)\n",
    "for sql_query in generated_sql:\n",
    "    print(sql_query)\n",
    "    sql_ans = engine.execute(sql_query).fetchall()\n",
    "    print(\"sql_ans:\", sql_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py2neo in /home/22842219/.local/lib/python3.8/site-packages (2021.2.3)\n",
      "Requirement already satisfied: six>=1.15.0 in /home/22842219/.local/lib/python3.8/site-packages (from py2neo) (1.16.0)\n",
      "Requirement already satisfied: pygments>=2.0.0 in /home/22842219/.local/lib/python3.8/site-packages (from py2neo) (2.14.0)\n",
      "Requirement already satisfied: interchange~=2021.0.4 in /home/22842219/.local/lib/python3.8/site-packages (from py2neo) (2021.0.4)\n",
      "Requirement already satisfied: monotonic in /usr/lib/python3/dist-packages (from py2neo) (1.5)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from py2neo) (2019.11.28)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from py2neo) (1.26.12)\n",
      "Requirement already satisfied: pansi>=2020.7.3 in /home/22842219/.local/lib/python3.8/site-packages (from py2neo) (2020.7.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from py2neo) (21.3)\n",
      "Requirement already satisfied: pytz in /home/22842219/.local/lib/python3.8/site-packages (from interchange~=2021.0.4->py2neo) (2022.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->py2neo) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install py2neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from py2neo import Graph\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/home/22842219/Desktop/phd/SemanticParser4Graph/application/config.ini')\n",
    "filenames = config[\"FILENAMES\"]\n",
    "\n",
    "neo4j_uri = filenames['neo4j_uri']\n",
    "neo4j_user = filenames['neo4j_user']\n",
    "neo4j_password = filenames['neo4j_password']\n",
    "graph = Graph(neo4j_uri, auth = (neo4j_user, neo4j_password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'min(singer.Age)': 25, 'avg(singer.Age)': 34.5, 'max(singer.Age)': 43}]\n"
     ]
    }
   ],
   "source": [
    "sql2cypher = \"MATCH (singer:`concert_singer.singer`) \\\n",
    "WHERE singer.Country = 'France' \\\n",
    "RETURN min(singer.Age),avg(singer.Age),max(singer.Age)\"\n",
    "cypher_res = graph.run(sql2cypher).data()\n",
    "print(cypher_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end-to-end translation from text-to-cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "########################## unifiedSKG learning process*******************\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from utils.configue import Configure\n",
    "from utils.training_arguments import WrappedSeq2SeqTrainingArguments\n",
    "\n",
    "from collections import OrderedDict\n",
    "import utils.tool\n",
    "from utils.dataset import TokenizedDataset\n",
    "from utils.trainer import EvaluateFriendlySeq2SeqTrainer\n",
    "\n",
    "# Huggingface realized the \"Seq2seqTrainingArguments\" which is the same with \"WrappedSeq2SeqTrainingArguments\"\n",
    "# in transformers==4.10.1 during our work.\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Configure.Get(training_args.cfg)\n",
    "print(training_args.cfg)\n",
    "print(args.bert, args.bert.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedSeq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "cfg=Salesforce/T5_base_prefix_spider_with_cell_value.cfg,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "generation_max_length=512,\n",
      "generation_num_beams=4,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "input_max_length=1536,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_weights_from=None,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./tmp/runs/Jan29_14-57-29_DEP24073.uniwa.uwa.edu.au,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "output_dir=./tmp,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=tmp,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./tmp,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'checkpoint-???' in args.bert.location:\n",
    "    args.bert.location = get_last_checkpoint(\n",
    "        os.path.dirname(args.bert.location.model_name_or_path))\n",
    "    logger.info(f\"Resolve model_name_or_path to {args.bert.location.model_name_or_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = None\n",
    "if os.path.isdir(\n",
    "        training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "\n",
    "os.makedirs(training_args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "./data/text2cypher\n",
      "seq2seq_construction.meta_tuning\n",
      "<utils.configue.Args object at 0x7f55056570d0>\n"
     ]
    }
   ],
   "source": [
    "print(not args.arg_paths)\n",
    "print(args.dataset.loader_path)  ## hey, attention!\n",
    "print(args.dataset.data_store_path)\n",
    "print(args.seq2seq.constructor)\n",
    "print(args.arg_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs will be train, dev, test or train, dev now.\n",
    "# We deprecate the k-fold cross-valid function \n",
    "# since it causes too many avoidable troubles.\n",
    "if not args.arg_paths:\n",
    "    cache_root = os.path.join('output', 'cache')\n",
    "    os.makedirs(cache_root, exist_ok=True)\n",
    "    raw_datasets_split: datasets.DatasetDict = datasets.load_dataset(path=args.dataset.loader_path,\n",
    "                                                                        cache_dir=args.dataset.data_store_path)\n",
    "    seq2seq_dataset_split: tuple = utils.tool.get_constructor(args.seq2seq.constructor)(args).to_seq2seq(\n",
    "        raw_datasets_split, cache_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_root: output/cache\n"
     ]
    }
   ],
   "source": [
    "cache_root = os.path.join('output', 'cache')\n",
    "print(f'cache_root: {cache_root}')\n",
    "os.makedirs(cache_root, exist_ok=True)\n",
    "meta_tuning_data = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset spider (./data/spider/spider/1.0.0/72b29069ca763a7a9cfbce069b3e457c8a5849f0621c7915bdda0ea45921ac6f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: spider, arg_path: META_TUNING/spider_with_cell.cfg, task_args: <utils.configue.Args object at 0x7f55280826a0>\n",
      "task_args.bert.location: t5-base\n",
      "task_args.dataset: <utils.configue.Args object at 0x7f553846f6a0>, task_args.dataset.loader_path: ./tasks/spider.py\n",
      "task_args.dataset.data_store_path: ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 402.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_raw_datasets_split: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types'],\n",
      "        num_rows: 4730\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types'],\n",
      "        num_rows: 705\n",
      "    })\n",
      "})\n",
      "task_args.seq2seq.constructor: seq2seq_construction.spider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg_path: META_TUNING/spider_with_cell.cfg\n",
      "meta_tuning_data: {'META_TUNING/spider_with_cell.cfg': (<seq2seq_construction.spider.TrainDataset object at 0x7f55334d0160>, <seq2seq_construction.spider.DevDataset object at 0x7f553846f160>)}\n",
      "seq2seq_dataset_split: (<seq2seq_construction.meta_tuning.TrainDataset object at 0x7f5520a7ab50>, <seq2seq_construction.meta_tuning.DevDataset object at 0x7f5520a7a190>, <seq2seq_construction.meta_tuning.TestDataset object at 0x7f5502bdd550>)\n"
     ]
    }
   ],
   "source": [
    "for task, arg_path in args.arg_paths:\n",
    "    task_args = Configure.Get(arg_path)\n",
    "    task_args.bert = args.bert\n",
    "    print(f'task: {task}, arg_path: {arg_path}, task_args: {task_args}')\n",
    "    print('task_args.bert.location:', task_args.bert.location)\n",
    "    print(f'task_args.dataset: {task_args.dataset}, task_args.dataset.loader_path: {task_args.dataset.loader_path}')\n",
    "    print(f'task_args.dataset.data_store_path: {task_args.dataset.data_store_path}')\n",
    "    task_raw_datasets_split: datasets.DatasetDict = datasets.load_dataset(\n",
    "        path=task_args.dataset.loader_path,\n",
    "        cache_dir=task_args.dataset.data_store_path)\n",
    "    print(f'task_raw_datasets_split: {task_raw_datasets_split}')\n",
    "\n",
    "    print(f'task_args.seq2seq.constructor: {task_args.seq2seq.constructor}')\n",
    "    task_seq2seq_dataset_split: tuple = utils.tool.get_constructor(task_args.seq2seq.constructor)(task_args).\\\n",
    "        to_seq2seq(task_raw_datasets_split, cache_root)\n",
    "\n",
    "    meta_tuning_data[arg_path] = task_seq2seq_dataset_split\n",
    "    print(f'arg_path: {arg_path}')\n",
    "    print(f'meta_tuning_data: {meta_tuning_data}')\n",
    "\n",
    "seq2seq_dataset_split: tuple = utils.tool.get_constructor(args.seq2seq.constructor)(args).\\\n",
    "    to_seq2seq(meta_tuning_data)\n",
    "print(f'seq2seq_dataset_split: {seq2seq_dataset_split}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<utils.configue.Args object at 0x7f55280826a0>\n",
      "<utils.configue.Args object at 0x7f552012fc70>\n"
     ]
    }
   ],
   "source": [
    "print(task_args)\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: <utils.configue.Args object at 0x7f552012fc70>, args.evaluate.tool: metrics.meta_tuning.evaluator\n",
      "evaluator: <metrics.meta_tuning.evaluator.EvaluateTool object at 0x7f550c7a8820>\n"
     ]
    }
   ],
   "source": [
    "print(f'args: {args}, args.evaluate.tool: {args.evaluate.tool}')\n",
    "evaluator = utils.tool.get_evaluator(args.evaluate.tool)(args)\n",
    "print(f'evaluator: {evaluator}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.model.name: unified.prefixtuning, args: <utils.configue.Args object at 0x7f552012fc70>\n",
      "prefix-tuning sequence length is 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/22842219/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.a085189636e38420df5e7bdf08ad1b86f1fe33c010079ca7f15437ff95f4fe2b\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /home/22842219/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /home/22842219/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/22842219/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.a085189636e38420df5e7bdf08ad1b86f1fe33c010079ca7f15437ff95f4fe2b\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/22842219/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.a085189636e38420df5e7bdf08ad1b86f1fe33c010079ca7f15437ff95f4fe2b\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /home/22842219/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Adding  < to the vocabulary\n",
      "Adding  <= to the vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(f'args.model.name: {args.model.name}, args: {args}')\n",
    "model = utils.tool.get_model(args.model.name)(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='t5-base', vocab_size=32100, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tokenizer = model.tokenizer\n",
    "model_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq_train_dataset: None\n"
     ]
    }
   ],
   "source": [
    "seq2seq_train_dataset, seq2seq_eval_dataset, seq2seq_test_dataset = None, None, None\n",
    "print(f'seq2seq_train_dataset: {seq2seq_train_dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(seq2seq_dataset_split): 3\n",
      "seq2seq_train_dataset: <seq2seq_construction.meta_tuning.TrainDataset object at 0x7f5520a7ab50>, seq2seq_eval_dataset: <seq2seq_construction.meta_tuning.DevDataset object at 0x7f5520a7a190>         seq2seq_test_dataset: <seq2seq_construction.meta_tuning.TestDataset object at 0x7f5502bdd550>\n"
     ]
    }
   ],
   "source": [
    "print(f'len(seq2seq_dataset_split): {len(seq2seq_dataset_split)}')\n",
    "if len(seq2seq_dataset_split) == 2:\n",
    "    seq2seq_train_dataset, seq2seq_eval_dataset = seq2seq_dataset_split\n",
    "elif len(seq2seq_dataset_split) == 3:\n",
    "    seq2seq_train_dataset, seq2seq_eval_dataset, seq2seq_test_dataset = seq2seq_dataset_split\n",
    "    print(f'seq2seq_train_dataset: {seq2seq_train_dataset}, seq2seq_eval_dataset: {seq2seq_eval_dataset} \\\n",
    "        seq2seq_test_dataset: {seq2seq_test_dataset}')\n",
    "else:\n",
    "    raise ValueError(\"Other split not support yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import TokenizedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: <utils.dataset.TokenizedDataset object at 0x7f55229c5340>\n",
      "eval_dataset: <utils.dataset.TokenizedDataset object at 0x7f55229c52e0>\n",
      "test_dataset: <utils.dataset.TokenizedDataset object at 0x7f55229c53a0>\n"
     ]
    }
   ],
   "source": [
    "# We wrap the \"string\" seq2seq data into \"tokenized tensor\".\n",
    "train_dataset = TokenizedDataset(args, training_args, model_tokenizer,\n",
    "                                    seq2seq_train_dataset) if seq2seq_train_dataset else None\n",
    "eval_dataset = TokenizedDataset(args, training_args, model_tokenizer,\n",
    "                                seq2seq_eval_dataset) if seq2seq_eval_dataset else None\n",
    "test_dataset = TokenizedDataset(args, training_args, model_tokenizer,\n",
    "                                seq2seq_test_dataset) if seq2seq_test_dataset else None\n",
    "print(f'train_dataset: {train_dataset}')\n",
    "print(f'eval_dataset: {eval_dataset}')\n",
    "print(f'test_dataset: {test_dataset}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping_callback: <transformers.trainer_callback.EarlyStoppingCallback object at 0x7f550c7a8d30>\n",
      "patience: 200\n"
     ]
    }
   ],
   "source": [
    "# Initialize our Trainer\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=args.seq2seq.patience if args.seq2seq.patience else 5)\n",
    "print(f'early_stopping_callback: {early_stopping_callback}')\n",
    "print(f'patience: {args.seq2seq.patience}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.trainer import EvaluateFriendlySeq2SeqTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq_eval_dataset: <seq2seq_construction.meta_tuning.DevDataset object at 0x7f5520a7a190>\n"
     ]
    }
   ],
   "source": [
    "print(f'seq2seq_eval_dataset: {seq2seq_eval_dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246\n",
    "!export WANDB_PROJECT='text-to-sql(text2cypher-24Jan)'\n",
    "!export WANDB_ENTITY=leamonzea929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wandb']\n",
      "True\n",
      "False\n",
      "uni-frame-for-knowledge-tabular-tasks\n",
      "./tmp\n",
      "sgtnew\n"
     ]
    }
   ],
   "source": [
    "print(training_args.report_to)\n",
    "print(training_args.local_rank <=0)\n",
    "print(\"MLFLOW_EXPERIMENT_ID\" in os.environ)\n",
    "print(os.getenv(\"WANDB_PROJECT\", \"uni-frame-for-knowledge-tabular-tasks\"))\n",
    "print(training_args.run_name)\n",
    "print(os.getenv(\"WANDB_ENTITY\", 'sgtnew'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer build successfully.\n"
     ]
    }
   ],
   "source": [
    "trainer = EvaluateFriendlySeq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    evaluator=evaluator,\n",
    "    # We name it \"evaluator\" while the hugging face call it \"Metric\",\n",
    "    # they are all f(predictions: List, references: List of dict) = eval_result: dict\n",
    "    tokenizer=model_tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    eval_examples=seq2seq_eval_dataset,\n",
    "    # wandb_run_dir=wandb.run.dir if \"wandb\" in training_args.report_to and training_args.local_rank <= 0 else None,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "print('Trainer build successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(training_args.load_weights_from)\n",
    "\n",
    "# Load model weights (for --do_train=False or post finetuning).\n",
    "if training_args.load_weights_from:\n",
    "    state_dict = torch.load(os.path.join(training_args.load_weights_from, transformers.WEIGHTS_NAME), map_location=\"cpu\")\n",
    "    trainer.model.load_state_dict(state_dict, strict=True)\n",
    "    # release memory\n",
    "    del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(args.load_multiple_prefix_module_weights_from)\n",
    "if args.load_multiple_prefix_module_weights_from:\n",
    "    reconstruct_state_dict = OrderedDict()\n",
    "\n",
    "    # load prefix modules\n",
    "    for task_name, module_weight_location in args.load_multiple_prefix_module_weights_from:\n",
    "        state_dict = torch.load(os.path.join(module_weight_location, transformers.WEIGHTS_NAME), map_location=\"cpu\")\n",
    "        MULTI_PREFIX_ATTR_NAME = \"multi_prefix\"\n",
    "        for weight_name, stored_tensor in state_dict.items():\n",
    "            if str(weight_name).startswith(\"pretrain_model\"):\n",
    "                continue  # skip the pretrained model and we will load a new one from another place\n",
    "            reconstruct_state_dict['{}.{}.{}'.format(MULTI_PREFIX_ATTR_NAME, \"_\".join(task_name.split(\"_\")[:-1]), weight_name)] = stored_tensor\n",
    "            # extract the prefix part and add them to dict\n",
    "\n",
    "    # give it into the model\n",
    "    trainer.model.load_state_dict(reconstruct_state_dict, strict=False)\n",
    "\n",
    "    # release memory\n",
    "    del reconstruct_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
