{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpath = '/home/22842219/Desktop/openSource/UnifiedSKGG-Cypher/data/text2cypher/schema.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Assumptions:\n",
    "#   1. cypher is correct\n",
    "#   2. only table name has alias\n",
    "#\n",
    "# val: number(float)/string(str)/cypher(dict)\n",
    "# col_unit: (agg_id, col_id, isDistinct(bool))\n",
    "# val_unit: (unit_op, col_unit1, col_unit2)\n",
    "# table_unit: (table_type, col_unit/cypher)\n",
    "# cond_unit: (not_op, op_id, val_unit, val1, val2)\n",
    "# condition: [cond_unit1, 'and'/'or', cond_unit2, ...]\n",
    "# cypher {\n",
    "#   'return': (isDistinct(bool), [(agg_id, val_unit), (agg_id, val_unit), ...])\n",
    "#   'match': {'table_units': [table_unit1, table_unit2, ...], 'conds': condition}\n",
    "#   'where': condition\n",
    "#   'orderBy': ('asc'/'desc', [val_unit1, val_unit2, ...])\n",
    "#   'having': condition\n",
    "#   'limit': None/limit value\n",
    "# }\n",
    "################################\n",
    "\n",
    "import json, re\n",
    "from nltk import word_tokenize\n",
    "\n",
    "alias_pattern = re.compile(r'(t[1-9]|[a-z])')\t\n",
    "labels_pattern = re.compile(r':`[a-z|A-Z].*`')\n",
    "\n",
    "CLAUSE_KEYWORDS = ('match', 'where', 'with', 'return', 'order', 'limit')\n",
    "\n",
    "WHERE_OPS = ('=', '>', '<', '>=', '<=', '<>', 'in', \"~'.*\")  # \"~.'.* -> like\"\n",
    "UNIT_OPS = ('none', '-', '+', \"*\", '/')\n",
    "AGG_OPS = ('none', 'max', 'min', 'count', 'sum', 'avg')\n",
    "TABLE_TYPE = {\n",
    "    'cypher': \"cypher\",\n",
    "    'table_unit': \"table_unit\",\n",
    "}\n",
    "\n",
    "COND_OPS = ('and', 'or')\n",
    "# CYPHER_OPS = ('intersect', 'union', 'except')\n",
    "ORDER_OPS = ('desc', 'asc')\n",
    "\n",
    "class Schema:\n",
    "    \"\"\"\n",
    "    Simple schema which maps graph nodes/edges and their properties to a unique identifier\n",
    "    \"\"\"\n",
    "    def __init__(self, schema):\n",
    "        self._schema = schema\n",
    "        self._idMap = self._map(self._schema)\n",
    "\n",
    "    @property\n",
    "    def schema(self):\n",
    "        return self._schema\n",
    "\n",
    "    @property\n",
    "    def idMap(self):\n",
    "        return self._idMap\n",
    "\n",
    "    def _map(self, schema):\n",
    "        idMap = {'*': \"__all__\"}\n",
    "        id = 1\n",
    "        for key, vals in schema.items():\n",
    "            for val in vals:\n",
    "                if val is None:\n",
    "                    continue\n",
    "                idMap[key.lower() + \".\" + val.lower()] = \"__\" + key.lower() + \".\" + val.lower() + \"__\"\n",
    "                id += 1\n",
    "\n",
    "        for key in schema:\n",
    "            idMap[key.lower()] = \"__\" + key.lower() + \"__\"\n",
    "            id += 1\n",
    "\n",
    "        return idMap\n",
    "\n",
    "\n",
    "def get_schema_from_json(fpath):\n",
    "    \"\"\"\n",
    "    Get the graph database's schema, which is a dict with the key in the format of :`db_name.tb_name`, e.g., :`department_management.head`\n",
    "    and list of property names as value\n",
    "    :param file: the schema json file\n",
    "    :return: schema dict\n",
    "    \"\"\"\n",
    "    schema = {}\n",
    "    with open(fpath) as reader:\n",
    "        data = json.load(reader)\n",
    "        for k, v in data.items():\n",
    "            schema[k]=v['property_names']\n",
    "\n",
    "    return schema\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    print(\"+++++++++++++++++++++++++++++++++tokenize++++++++++++++++++++++++++++++\")\n",
    "    string = str(string) #SELECT count(*) FROM singer --> match (n:`singer.singer`) return count(*)\n",
    "    string = string.replace(\"\\'\", \"\\\"\")  # ensures all string values wrapped by \"\" problem??\n",
    "    quote_idxs = [idx for idx, char in enumerate(string) if char == '\"'] #[]\n",
    "    assert len(quote_idxs) % 2 == 0, \"Unexpected quote\"\n",
    "\n",
    "    # keep string value as token\n",
    "    vals = {}\n",
    "    for i in range(len(quote_idxs)-1, -1, -2):\n",
    "        qidx1 = quote_idxs[i-1]\n",
    "        qidx2 = quote_idxs[i]\n",
    "        val = string[qidx1: qidx2+1]\n",
    "        key = \"__val_{}_{}__\".format(qidx1, qidx2)\n",
    "        string = string[:qidx1] + key + string[qidx2+1:]\n",
    "        vals[key] = val\n",
    " \n",
    "    toks=[]\n",
    "    chunk_start_id=0\n",
    "    for id, tok in enumerate(word_tokenize(string)):\n",
    "        # print(f'id: {id}, tok: {tok}, chunk_start_id: {chunk_start_id}, {chunk_start_id+3}')\n",
    "        if tok==':':\n",
    "            chunk_start_id = id\n",
    "            toks.append(''.join(word_tokenize(string)[id:id+4]).lower())\n",
    "        elif  chunk_start_id==0:\n",
    "            toks.append(tok.lower())\n",
    "        elif  id > chunk_start_id+3:\n",
    "            toks.append(tok.lower())\n",
    "            \n",
    "        if tok=='[':\n",
    "            toks.pop()\n",
    "            toks.pop()\n",
    "            toks.append('-[')\n",
    "        if word_tokenize(string)[id-1]==']':\n",
    "            toks.pop()\n",
    "            toks.pop()\n",
    "            toks.append(']-')\n",
    "       \n",
    "    # replace with string value token\n",
    "    for i in range(len(toks)):\n",
    "        if toks[i] in vals:\n",
    "            toks[i] = vals[toks[i]]\n",
    "\n",
    "    # find if there exists !=, >=, <=\n",
    "    eq_idxs = [idx for idx, tok in enumerate(toks) if tok == \"=\"]\n",
    "    eq_idxs.reverse()\n",
    "    prefix = ('!', '>', '<')\n",
    "    for eq_idx in eq_idxs:\n",
    "        pre_tok = toks[eq_idx-1]\n",
    "        if pre_tok in prefix:\n",
    "            toks = toks[:eq_idx-1] + [pre_tok + \"=\"] + toks[eq_idx+1: ]\n",
    "    return toks\n",
    "\n",
    "\n",
    "def scan_alias(toks):\n",
    "    \"\"\"Scan the index of 'as' and build the map for all alias\"\"\"\n",
    "    alias_idxs = [idx for idx, tok in enumerate(toks) if tok.startswith(':`')]\n",
    "    alias = {}\n",
    "    for idx in alias_idxs:\n",
    "        if re.fullmatch(alias_pattern, toks[idx-1].lower()):\n",
    "            alias[toks[idx - 1]] = toks[idx]\n",
    "    return alias\n",
    "\n",
    "\n",
    "def get_tables_with_alias(schema, toks):\n",
    "    tables = scan_alias(toks)\n",
    "    print(f'scan tables: {tables}')\n",
    "    existing = list(tables.values())\n",
    "    for tok in toks:\n",
    "        if tok.startswith(':`'):\n",
    "            domain = tok.split('.')[0]\n",
    "    for key in schema:\n",
    "        if key.startswith(domain):\n",
    "            assert key not in tables, \"Alias {} has the same name in table\".format(key)\n",
    "            alias = key.strip('`').split('.')[-1] \n",
    "            if key not in existing:\n",
    "                tables[alias] = key\n",
    "    return tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++tokenize++++++++++++++++++++++++++++++\n",
      "toks: ['match', '(', 't1', ':`concert_singer.concert`', ')', '-[', ']-', '(', 't2', ':`concert_singer.stadium`', ')', 'with', 't2.name', 'as', 'name', ',', 'count', '(', 't1.stadium_id', ')', 'as', 'count', 'return', 'name', ',', 'count']\n"
     ]
    }
   ],
   "source": [
    "test_cypher = \"MATCH (T1:`concert_singer.concert`)-[]-(T2:`concert_singer.stadium`)\\nWITH T2.Name AS Name, count(T1.Stadium_ID) AS count\\nRETURN Name,count\"\n",
    "                   #\"match (n:`concert_singer.singer`) return count(*)\"\n",
    "toks = tokenize(test_cypher)\n",
    "print(f'toks: {toks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++tokenize++++++++++++++++++++++++++++++\n",
      "scan tables: {'t1': ':`concert_singer.concert`', 't2': ':`concert_singer.stadium`'}\n",
      "tables_with_alias: {'t1': ':`concert_singer.concert`', 't2': ':`concert_singer.stadium`', 'singer': ':`concert_singer.singer`'}\n"
     ]
    }
   ],
   "source": [
    "schema = Schema(get_schema_from_json(fpath))\n",
    "# from process_cypher import get_tables_with_alias\n",
    "tables_with_alias = get_tables_with_alias(schema.schema, tokenize(test_cypher))  #schema.schema: dict\n",
    "print(f'tables_with_alias: {tables_with_alias}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_col(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    \"\"\"\n",
    "        :returns next idx, column id\n",
    "    \"\"\"\n",
    "    tok = toks[start_idx]\n",
    "    if tok == \"*\":\n",
    "        return start_idx + 1, schema.idMap[tok]\n",
    "\n",
    "    if '.' in tok:  # if token is a composite\n",
    "        alias, col = tok.split('.')\n",
    "        key = tables_with_alias[alias] + \".\" + col\n",
    "        return start_idx+1, schema.idMap[key]\n",
    "\n",
    "    assert default_tables is not None and len(default_tables) > 0, \"Default tables should not be None or empty\"\n",
    "\n",
    "    for alias in default_tables:\n",
    "        table = tables_with_alias[alias]\n",
    "        if tok in schema.schema[table]:\n",
    "            key = table + \".\" + tok\n",
    "            return start_idx+1, schema.idMap[key]\n",
    "\n",
    "    assert False, \"Error col: {}\".format(tok)\n",
    "\n",
    "\n",
    "def parse_col_unit(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    \"\"\"\n",
    "        :returns next idx, (agg_op id, col_id)\n",
    "    \"\"\"\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    isBlock = False\n",
    "    isDistinct = False\n",
    "    if toks[idx] == '(':\n",
    "        isBlock = True\n",
    "        idx += 1\n",
    "\n",
    "    if toks[idx] in AGG_OPS:\n",
    "        agg_id = AGG_OPS.index(toks[idx])\n",
    "        idx += 1\n",
    "        assert idx < len_ and toks[idx] == '('\n",
    "        idx += 1\n",
    "        if toks[idx] == \"distinct\":\n",
    "            idx += 1\n",
    "            isDistinct = True\n",
    "        idx, col_id = parse_col(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        assert idx < len_ and toks[idx] == ')'\n",
    "        idx += 1\n",
    "        return idx, (agg_id, col_id, isDistinct)\n",
    "\n",
    "    if toks[idx] == \"distinct\":\n",
    "        idx += 1\n",
    "        isDistinct = True\n",
    "    agg_id = AGG_OPS.index(\"none\")\n",
    "    idx, col_id = parse_col(toks, idx, tables_with_alias, schema, default_tables)\n",
    "\n",
    "    if isBlock:\n",
    "        assert toks[idx] == ')'\n",
    "        idx += 1  # skip ')'\n",
    "\n",
    "    return idx, (agg_id, col_id, isDistinct)\n",
    "\n",
    "\n",
    "def parse_val_unit(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    isBlock = False\n",
    "    if toks[idx] == '(':\n",
    "        isBlock = True\n",
    "        idx += 1\n",
    "\n",
    "    col_unit1 = None\n",
    "    col_unit2 = None\n",
    "    unit_op = UNIT_OPS.index('none')\n",
    "\n",
    "    idx, col_unit1 = parse_col_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    if idx < len_ and toks[idx] in UNIT_OPS:\n",
    "        unit_op = UNIT_OPS.index(toks[idx])\n",
    "        idx += 1\n",
    "        idx, col_unit2 = parse_col_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "\n",
    "    if isBlock:\n",
    "        assert toks[idx] == ')'\n",
    "        idx += 1  # skip ')'\n",
    "\n",
    "    return idx, (unit_op, col_unit1, col_unit2)\n",
    "\n",
    "\n",
    "def parse_table_unit(toks, start_idx, tables_with_alias, schema):\n",
    "    \"\"\"\n",
    "        :returns next idx, table id, table name\n",
    "    \"\"\"\n",
    "\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    \n",
    "    key = tables_with_alias[toks[idx]]\n",
    "    print(f'parse table unit, idx: {idx}, len_: {len_}, toks[idx]: {toks[idx]}')\n",
    "    print(f'key: {key}, schema.idMap[key]: {schema.idMap[key]}')\n",
    "    if re.fullmatch(alias_pattern, toks[idx]):\n",
    "        idx += 2\n",
    "    else:\n",
    "        idx += 1\n",
    "    \n",
    "    return idx, schema.idMap[key], key\n",
    "\n",
    "\n",
    "def parse_value(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "\n",
    "    isBlock = False\n",
    "    if toks[idx] == '(':\n",
    "        isBlock = True\n",
    "        idx += 1\n",
    "\n",
    "    if toks[idx] == 'return':\n",
    "        idx, val = parse_cypher(toks, idx, tables_with_alias, schema)\n",
    "    elif \"\\\"\" in toks[idx]:  # token is a string value\n",
    "        val = toks[idx]\n",
    "        idx += 1\n",
    "    else:\n",
    "        try:\n",
    "            val = float(toks[idx])\n",
    "            idx += 1\n",
    "        except:\n",
    "            end_idx = idx\n",
    "            while end_idx < len_ and toks[end_idx] != ',' and toks[end_idx] != ')'\\\n",
    "                and toks[end_idx] != 'and' and toks[end_idx] not in CLAUSE_KEYWORDS and toks[end_idx] not in JOIN_KEYWORDS:\n",
    "                    end_idx += 1\n",
    "\n",
    "            idx, val = parse_col_unit(toks[start_idx: end_idx], 0, tables_with_alias, schema, default_tables)\n",
    "            idx = end_idx\n",
    "\n",
    "    if isBlock:\n",
    "        assert toks[idx] == ')'\n",
    "        idx += 1\n",
    "\n",
    "    return idx, val\n",
    "\n",
    "\n",
    "def parse_condition(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    conds = []\n",
    "\n",
    "    while idx < len_:\n",
    "        idx, val_unit = parse_val_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        not_op = False\n",
    "        if toks[idx] == 'not':\n",
    "            not_op = True\n",
    "            idx += 1\n",
    "\n",
    "        assert idx < len_ and toks[idx] in WHERE_OPS, \"Error condition: idx: {}, tok: {}\".format(idx, toks[idx])\n",
    "        op_id = WHERE_OPS.index(toks[idx])\n",
    "        idx += 1\n",
    "        val1 = val2 = None\n",
    "        if op_id == WHERE_OPS.index('between'):  # between..and... special case: dual values\n",
    "            idx, val1 = parse_value(toks, idx, tables_with_alias, schema, default_tables)\n",
    "            assert toks[idx] == 'and'\n",
    "            idx += 1\n",
    "            idx, val2 = parse_value(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        else:  # normal case: single value\n",
    "            idx, val1 = parse_value(toks, idx, tables_with_alias, schema, default_tables)\n",
    "            val2 = None\n",
    "\n",
    "        conds.append((not_op, op_id, val_unit, val1, val2))\n",
    "\n",
    "        if idx < len_ and (toks[idx] in CLAUSE_KEYWORDS or toks[idx] in (\")\", \";\") or toks[idx] in JOIN_KEYWORDS):\n",
    "            break\n",
    "\n",
    "        if idx < len_ and toks[idx] in COND_OPS:\n",
    "            conds.append(toks[idx])\n",
    "            idx += 1  # skip and/or\n",
    "\n",
    "    return idx, conds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_match(toks, start_idx, tables_with_alias, schema):\n",
    "\n",
    "    assert 'match' in toks[start_idx:], \"'match' not found\"\n",
    "\n",
    "    len_ = len(toks)\n",
    "    idx = toks.index('match', start_idx) + 2\n",
    "    default_tables = []\n",
    "    table_units = []\n",
    "    conds = []\n",
    "\n",
    "    while idx < len_:\n",
    "        print(idx, toks[idx],len_)\n",
    "        if toks[idx] == 'union': # TODO\n",
    "            idx, cypher = parse_cypher(toks, idx, tables_with_alias, schema)\n",
    "            table_units.append((TABLE_TYPE['cypher'], cypher))\n",
    "        else:\n",
    "            if idx < len_ and toks[idx] == '-[' and toks[idx+1] ==']-':\n",
    "                idx += 3  # skip edge and the right bracket of tail node pattern\n",
    "            if re.fullmatch(labels_pattern, toks[idx]):\n",
    "                table_unit = schema.idMap(toks[idx])\n",
    "                table_units.append((TABLE_TYPE['table_unit'],table_unit))\n",
    "                idx+=1\n",
    "            \n",
    "        if re.fullmatch(alias_pattern, toks[idx]):\n",
    "            idx, table_unit, table_name = parse_table_unit(toks, idx, tables_with_alias, schema)\n",
    "            table_units.append((TABLE_TYPE['table_unit'],table_unit))\n",
    "            default_tables.append(table_name)\n",
    "            idx+=1\n",
    "\n",
    "        if idx < len_ and toks[idx] in CLAUSE_KEYWORDS:\n",
    "            break\n",
    "\n",
    "    return idx, table_units, conds, default_tables\n",
    "\n",
    "\n",
    "def parse_with_as(toks, start_idx, tables_with_alias, schema, default_tables):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "\n",
    "    if idx >= len_ or toks[idx] != 'with':\n",
    "        return idx, []\n",
    "\n",
    "    idx += 1\n",
    "    idx, conds = parse_condition(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    return idx, conds\n",
    "\n",
    "def parse_where(toks, start_idx, tables_with_alias, schema, default_tables):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "\n",
    "    if idx >= len_ or toks[idx] != 'where':\n",
    "        return idx, []\n",
    "\n",
    "    idx += 1\n",
    "    idx, conds = parse_condition(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    return idx, conds\n",
    "\n",
    "\n",
    "\n",
    "def parse_cypher(toks, start_idx, tables_with_alias, schema):\n",
    "    idx = start_idx \n",
    "    cypher = {}\n",
    "\n",
    "    # parse match clause in order to get default tables\n",
    "    idx, table_units, conds, default_tables = parse_match(toks, start_idx, tables_with_alias, schema)\n",
    "    cypher['match'] = {'table_units': table_units, 'conds': conds}\n",
    "\n",
    "    print(f'toks: {toks}')\n",
    "    print(f'tables_with_alis: {tables_with_alias}')\n",
    "    print(f'match start_idx: {start_idx}, match_end_idx: {idx}' ) \n",
    "    print(f'table_units: {table_units}, conds: {conds}, default_tables: {default_tables}')\n",
    "    print(f\"cypher[match]: {cypher['match']}\")\n",
    "\n",
    "\n",
    "    # with-as clause\n",
    "    idx, with_conds = parse_with_as(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    cypher['with'] = with_conds\n",
    "    print(\"with_conds:\", with_conds)\n",
    "\n",
    "    # where clause\n",
    "    idx, where_conds = parse_where(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    cypher['where'] = where_conds #[]\n",
    "    print('idx in where:', idx) # 7\n",
    "    print(\"where_conds:\", where_conds)\n",
    "\n",
    "\n",
    "    return idx, cypher\n",
    "\n",
    "\n",
    "\n",
    "def get_cypher(schema, query):\n",
    "    toks = tokenize(query)\n",
    "    tables_with_alias = get_tables_with_alias(schema.schema, toks)  #schema.schema: dict\n",
    "    _, cypher = parse_cypher(toks, 0, tables_with_alias, schema)\n",
    "    return cypher\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['match', '(', 't1', ':`concert_singer.concert`', ')', '-[', ']-', '(', 't2', ':`concert_singer.stadium`', ')', 'with', 't2.name', 'as', 'name', ',', 'count', '(', 't1.stadium_id', ')', 'as', 'count', 'return', 'name', ',', 'count']\n",
      "with\n"
     ]
    }
   ],
   "source": [
    "print(toks)\n",
    "print(toks[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 t1 26\n",
      "parse table unit, idx: 2, len_: 26, toks[idx]: t1\n",
      "key: :`concert_singer.concert`, schema.idMap[key]: __:`concert_singer.concert`__\n",
      "5 -[ 26\n",
      "parse table unit, idx: 8, len_: 26, toks[idx]: t2\n",
      "key: :`concert_singer.stadium`, schema.idMap[key]: __:`concert_singer.stadium`__\n",
      "toks: ['match', '(', 't1', ':`concert_singer.concert`', ')', '-[', ']-', '(', 't2', ':`concert_singer.stadium`', ')', 'with', 't2.name', 'as', 'name', ',', 'count', '(', 't1.stadium_id', ')', 'as', 'count', 'return', 'name', ',', 'count']\n",
      "tables_with_alis: {'t1': ':`concert_singer.concert`', 't2': ':`concert_singer.stadium`', 'singer': ':`concert_singer.singer`'}\n",
      "match start_idx: 0, match_end_idx: 11\n",
      "table_units: [('table_unit', '__:`concert_singer.concert`__'), ('table_unit', '__:`concert_singer.stadium`__')], conds: [], default_tables: [':`concert_singer.concert`', ':`concert_singer.stadium`']\n",
      "cypher[match]: {'table_units': [('table_unit', '__:`concert_singer.concert`__'), ('table_unit', '__:`concert_singer.stadium`__')], 'conds': []}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Error condition: idx: 13, tok: as",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# toks = tokenize('MATCH (singer:`concert_singer.singer`)\\nRETURN singer.Song_Name,singer.Song_release_year\\nORDER BY singer.Age\\nLIMIT 1')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# print(f'toks: {toks}')\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m _, cypher \u001b[39m=\u001b[39m parse_cypher(toks, \u001b[39m0\u001b[39;49m, tables_with_alias, schema)\n",
      "Cell \u001b[0;32mIn[6], line 76\u001b[0m, in \u001b[0;36mparse_cypher\u001b[0;34m(toks, start_idx, tables_with_alias, schema)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcypher[match]: \u001b[39m\u001b[39m{\u001b[39;00mcypher[\u001b[39m'\u001b[39m\u001b[39mmatch\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[39m# with-as clause\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m idx, with_conds \u001b[39m=\u001b[39m parse_with_as(toks, idx, tables_with_alias, schema, default_tables)\n\u001b[1;32m     77\u001b[0m cypher[\u001b[39m'\u001b[39m\u001b[39mwith\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m with_conds\n\u001b[1;32m     78\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mwith_conds:\u001b[39m\u001b[39m\"\u001b[39m, with_conds)\n",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m, in \u001b[0;36mparse_with_as\u001b[0;34m(toks, start_idx, tables_with_alias, schema, default_tables)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m idx, []\n\u001b[1;32m     43\u001b[0m idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 44\u001b[0m idx, conds \u001b[39m=\u001b[39m parse_condition(toks, idx, tables_with_alias, schema, default_tables)\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m idx, conds\n",
      "Cell \u001b[0;32mIn[5], line 153\u001b[0m, in \u001b[0;36mparse_condition\u001b[0;34m(toks, start_idx, tables_with_alias, schema, default_tables)\u001b[0m\n\u001b[1;32m    150\u001b[0m     not_op \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 153\u001b[0m \u001b[39massert\u001b[39;00m idx \u001b[39m<\u001b[39m len_ \u001b[39mand\u001b[39;00m toks[idx] \u001b[39min\u001b[39;00m WHERE_OPS, \u001b[39m\"\u001b[39m\u001b[39mError condition: idx: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, tok: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(idx, toks[idx])\n\u001b[1;32m    154\u001b[0m op_id \u001b[39m=\u001b[39m WHERE_OPS\u001b[39m.\u001b[39mindex(toks[idx])\n\u001b[1;32m    155\u001b[0m idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Error condition: idx: 13, tok: as"
     ]
    }
   ],
   "source": [
    "# toks = tokenize('MATCH (singer:`concert_singer.singer`)\\nRETURN singer.Song_Name,singer.Song_release_year\\nORDER BY singer.Age\\nLIMIT 1')\n",
    "# print(f'toks: {toks}')\n",
    "_, cypher = parse_cypher(toks, 0, tables_with_alias, schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
