- Pricai-exps

#### run model on text2cypher

## Text-to-Cypher experiments on Spider-to-SpiderGraph
CodeT5-base: fine-tuning
```
export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246
export WANDB_PROJECT='text2cypher-spider-pricai-codet5-finetune'
export WANDB_ENTITY=leamonzea929
python -m torch.distributed.launch               --nproc_per_node 1                --master_port 1234 train.py                --seed 2                --cfg Salesforce/CodeT5_base_finetune_text2cypher.cfg                --run_name CodeT5_base_finetune_text2cypher          --logging_strategy steps                --logging_first_step true                --logging_steps 500                --evaluation_strategy steps                --eval_steps 400                --metric_for_best_model avr                --greater_is_better true                --save_strategy steps                --save_steps 400                --save_total_limit 1                --load_best_model_at_end                --gradient_accumulation_steps 8                --num_train_epochs 100                --adafactor true                --learning_rate 1e-4                --do_train                --do_eval                --do_predict                --predict_with_generate                --output_dir output/CodeT5_base_finetune_text2cypher           --overwrite_output_dir            --per_device_train_batch_size 2                --per_device_eval_batch_size 8                --generation_num_beams 2               --generation_max_length 512                --input_max_length 512                --ddp_find_unused_parameters true  
```

CodeT5-base: prefix-tuning
```
export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246
export WANDB_PROJECT='text2cypher-spider-pricai-codet5-prefix'
export WANDB_ENTITY=leamonzea929
python -m torch.distributed.launch               --nproc_per_node 1                --master_port 1234 train.py                --seed 2                --cfg Salesforce/CodeT5_base_prefix_text2cypher.cfg                --run_name CodeT5_base_prefix_text2cypher          --logging_strategy steps                --logging_first_step true                --logging_steps 500                --evaluation_strategy steps                --eval_steps 400                --metric_for_best_model avr                --greater_is_better true                --save_strategy steps                --save_steps 400                --save_total_limit 1                --load_best_model_at_end                --gradient_accumulation_steps 8                --num_train_epochs 100                --adafactor true                --learning_rate 1e-4                --do_train                --do_eval                --do_predict                --predict_with_generate                --output_dir output/CodeT5_base_prefix_text2cypher           --overwrite_output_dir            --per_device_train_batch_size 2                --per_device_eval_batch_size 8                --generation_num_beams 2               --generation_max_length 512                --input_max_length 512                --ddp_find_unused_parameters true  

```



## Text-to-SQL experiments on Spider
CodeT5-base: fine-tuning
```
export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246
export WANDB_PROJECT='text2sql-spider-pricai-codet5-finetune'
export WANDB_ENTITY=leamonzea929


CodeT5-base: prefix-tuning
```
export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246
export WANDB_PROJECT='text2sql-spider-pricai-codet5-prefix'
export WANDB_ENTITY=leamonzea929


```
