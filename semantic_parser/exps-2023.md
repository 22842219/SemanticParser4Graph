- Date: Jan, 2023

#### run models on the whole spider dataset 

```
export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246
export WANDB_PROJECT='text-to-sql(spider)'
export WANDB_ENTITY=leamonzea929
python -m torch.distributed.launch     --nproc_per_node 1     --master_port 1234 train.py     --seed 2     --cfg Salesforce/T5_base_prefix_spider_with_cell_value.cfg     --run_name T5_base_prefix_spider_with_cell_value     --logging_strategy steps     --logging_first_step true     --logging_steps 4     --evaluation_strategy steps     --eval_steps 500     --metric_for_best_model avr     --greater_is_better true     --save_strategy steps     --save_steps 500     --save_total_limit 1     --load_best_model_at_end     --gradient_accumulation_steps 8     --num_train_epochs 400     --adafactor true     --learning_rate 1e-4     --do_train     --do_eval     --do_predict     --predict_with_generate     --output_dir output/T5_base_prefix_spider_with_cell_value     --per_device_train_batch_size 2     --per_device_eval_batch_size 8     --generation_num_beams 1     --generation_max_length 512     --input_max_length 512     --ddp_find_unused_parameters true  

***** Running training *****
  Num examples = 7000
  Num Epochs = 400
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 174800
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 339
  Continuing training from global step 148500
  Will skip the first 339 epochs then the first 2856 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.


+++++++++++++++++++++++++++++++++tokenize++++++++++++++++++++++++++++++
return toks: ['select', 'count', '(', '*', ')', 'from', 'singer']
query: SELECT count(*) FROM singer, toks: ['select', 'count', '(', '*', ')', 'from', 'singer']
tables_with_alias: {'stadium': 'stadium', 'singer': 'singer', 'concert': 'concert', 'singer_in_concert': 'singer_in_concert'}
parse_sql/idx: 0
len_: 7
heyyyy,  7 __singer__ singer
TABLE_TYPE['table_unit']: __singer__
 from_end_idx 7
 table_units [('table_unit', '__singer__')]
 conds  []
 default_tables  ['singer']
heyyy select/toks[idx] count
agg_id: 3
val_unit, idx: (0, (0, '__all__', False), None) 5
select_col_units: (False, [(3, (0, (0, '__all__', False), None))])
idx: 7
idx in where: 7
where_conds: []
groupBy idx: 7
group_col_units: []
having_conds: []
order_col_units: []
limit_val: None
idx: 7
idx: 7
sql: {'from': {'table_units': [('table_unit', '__singer__')], 'conds': []}, 'select': (False, [(3, (0, (0, '__all__', False), None))]), 'where': [], 'groupBy': [], 'having': [], 'orderBy': [], 'limit': None}
Traceback (most recent call last):
***** predict metrics *****
  epoch                                                =      400.0
  predict_META_TUNING/spider_with_cell.cfg/exact_match =     0.5706
  predict_META_TUNING/spider_with_cell.cfg/exec        =     0.5822
  predict_avr                                          =     0.5764
  predict_loss                                         =      0.285
  predict_runtime                                      = 0:09:43.27
  predict_samples                                      =       1034
  predict_samples_per_second                           =      1.773
wandb: Processing terminal ouput (stdout)...
wandb: Done.

wandb: Waiting for W&B process to finish, PID 487059
wandb: Program ended successfully.
wandb:                                                                                
wandb: Find user logs for this run at: /home/22842219/Desktop/openSource/UnifiedSKGG/wandb/run-20230113_182039-28s1xjuy/logs/debug.log
wandb: Find internal logs for this run at: /home/22842219/Desktop/openSource/UnifiedSKGG/wandb/run-20230113_182039-28s1xjuy/logs/debug-internal.log
wandb: Run summary:
wandb:                                                   train/loss 0.0588
wandb:                                          train/learning_rate 0.0
wandb:                                                  train/epoch 400.0
wandb:                                            train/global_step 174800
wandb:                                                     _runtime 48206
wandb:                                                   _timestamp 1673653445
wandb:                                                        _step 2597
wandb:                                                    eval/loss 0.28496
wandb:                                                 eval/runtime 569.4107
wandb:                                      eval/samples_per_second 1.816
wandb:            eval/META_TUNING/spider_with_cell.cfg/exact_match 0.5706
wandb:                   eval/META_TUNING/spider_with_cell.cfg/exec 0.58221
wandb:                                                     eval/avr 0.5764
wandb:                                          train/train_runtime 47014.2749
wandb:                               train/train_samples_per_second 59.556
wandb:                                 train/train_steps_per_second 3.718
wandb:                                             train/total_flos 1.959236847783641e+18
wandb:                                             train/train_loss 0.0031
wandb:                                           train/predict_loss 0.28496
wandb:                                        train/predict_runtime 583.2772
wandb:                             train/predict_samples_per_second 1.773
wandb:   train/predict_META_TUNING/spider_with_cell.cfg/exact_match 0.5706
wandb:          train/predict_META_TUNING/spider_with_cell.cfg/exec 0.58221
wandb:                                            train/predict_avr 0.5764
wandb: Run history:
wandb:                                                   train/loss ▃▆▄█▅▆▅▅▂▅▆▄▄▂▃▆▆▂▅▃▂▅▁▂▃▅▂▁▅▇▁▄▆▅▂▇▇▄▅▆
wandb:                                          train/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:                                                  train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                                            train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                                                     _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                                                   _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                                                        _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                                                    eval/loss ▃▅▁▂▆▄▇█▄█▆▅▆▅▆▅▃▅▄▅▆
wandb:                                                 eval/runtime ▁▄▄▆▄▃▅▆▇█▅▆▆▄▃▅▆▃▅▁▁
wandb:                                      eval/samples_per_second █▅▄▃▅▆▄▃▂▁▄▃▃▅▆▄▃▆▄▇█
wandb:            eval/META_TUNING/spider_with_cell.cfg/exact_match ▄▄▅▅▅▄▃▂▁▅▅▇▄▅██▇▇▇▇█
wandb:                   eval/META_TUNING/spider_with_cell.cfg/exec ▂█▄▄▄▅▂▁▁▄▅▇▃▅█▇▅▇▅▅█
wandb:                                                     eval/avr ▃▅▄▅▅▄▂▂▁▄▅▇▄▅██▆▇▆▆█
wandb:                                          train/train_runtime ▁
wandb:                               train/train_samples_per_second ▁
wandb:                                 train/train_steps_per_second ▁
wandb:                                             train/total_flos ▁
wandb:                                             train/train_loss ▁
wandb:                                           train/predict_loss ▁
wandb:                                        train/predict_runtime ▁
wandb:                             train/predict_samples_per_second ▁
wandb:   train/predict_META_TUNING/spider_with_cell.cfg/exact_match ▁
wandb:          train/predict_META_TUNING/spider_with_cell.cfg/exec ▁
wandb:                                            train/predict_avr ▁
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 22 other file(s)
wandb: 
```

#### run model on the subset of spider

```
export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246
export WANDB_PROJECT='text-to-sql(subset-spider-0117)'
export WANDB_ENTITY=leamonzea929
python -m torch.distributed.launch     --nproc_per_node 1     --master_port 1234 train.py     --seed 2     --cfg Salesforce/T5_base_prefix_spider_with_cell_value.cfg     --run_name T5_base_prefix_spider_with_cell_value     --logging_strategy steps     --logging_first_step true     --logging_steps 4     --evaluation_strategy steps     --eval_steps 500     --metric_for_best_model avr     --greater_is_better true     --save_strategy steps     --save_steps 500     --save_total_limit 1     --load_best_model_at_end     --gradient_accumulation_steps 8     --num_train_epochs 400     --adafactor true     --learning_rate 1e-4     --do_train     --do_eval     --do_predict     --predict_with_generate     --output_dir output/T5_base_prefix_spider_with_cell_value   --overwrite_output_dir  --per_device_train_batch_size 2     --per_device_eval_batch_size 8     --generation_num_beams 4    --generation_max_length 512     --input_max_length 512     --ddp_find_unused_parameters true  


---------------log------------------------
Loading model from output/T5_base_prefix_spider_with_cell_value/checkpoint-148500.

***** predict metrics *****
  epoch                                                =      400.0
  predict_META_TUNING/spider_with_cell.cfg/exact_match =     0.6596
  predict_META_TUNING/spider_with_cell.cfg/exec        =     0.6879
  predict_avr                                          =     0.6738
  predict_loss                                         =     0.3254
  predict_runtime                                      = 0:10:19.26
  predict_samples                                      =        705
  predict_samples_per_second                           =      1.138
wandb: Processing terminal ouput (stdout)...
wandb: Done.

wandb: Waiting for W&B process to finish, PID 113391
wandb: Program ended successfully.
wandb:                                                                                
wandb: Find user logs for this run at: /home/22842219/Desktop/openSource/UnifiedSKGG-subSpider/wandb/run-20230120_101159-gprxnfoo/logs/debug.log
wandb: Find internal logs for this run at: /home/22842219/Desktop/openSource/UnifiedSKGG-subSpider/wandb/run-20230120_101159-gprxnfoo/logs/debug-internal.log
wandb: Run summary:
wandb:                                                   train/loss 0.0081
wandb:                                          train/learning_rate 0.0
wandb:                                                  train/epoch 400.0
wandb:                                            train/global_step 118000
wandb:                                                     _runtime 152903
wandb:                                                   _timestamp 1674333622
wandb:                                                        _step 8192
wandb:                                                    eval/loss 0.32536
wandb:                                                 eval/runtime 616.232
wandb:                                      eval/samples_per_second 1.144
wandb:            eval/META_TUNING/spider_with_cell.cfg/exact_match 0.65957
wandb:                   eval/META_TUNING/spider_with_cell.cfg/exec 0.68794
wandb:                                                     eval/avr 0.67376
wandb:                                          train/train_runtime 151639.6541
wandb:                               train/train_samples_per_second 12.477
wandb:                                 train/train_steps_per_second 0.778
wandb:                                             train/total_flos 1.3238811123882394e+18
wandb:                                             train/train_loss 0.00345
wandb:                                           train/predict_loss 0.32536
wandb:                                        train/predict_runtime 619.2613
wandb:                             train/predict_samples_per_second 1.138
wandb:   train/predict_META_TUNING/spider_with_cell.cfg/exact_match 0.65957
wandb:          train/predict_META_TUNING/spider_with_cell.cfg/exec 0.68794
wandb:                                            train/predict_avr 0.67376
wandb: Run history:
wandb:                                                   train/loss ▅▆▅▄█▄▄▂▃▆▅▃▇▄▅▁▃▅▅▆▅▂█▃▂▄▃▁▄▂▄▁▃▅▃▄▂▂▆▂
wandb:                                          train/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:                                                  train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                                            train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                                                     _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                                                   _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                                                        _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                                                    eval/loss ▃▆▆▆▄▃▆▆▆▇▅▆▆▅▆▆▆▇▆▇▇▅▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▁
wandb:                                                 eval/runtime █▅▅▅▄▄▅▃▁▃▄▃▅▃▃▂▃▂▄▄▃▃▃▄▃▃▄▂▄▃▄▃▅▄▄▃▃▃▃▅
wandb:                                      eval/samples_per_second ▁▄▄▄▅▅▄▅█▆▅▅▄▆▅▆▆▇▅▅▆▆▆▅▅▆▅▇▅▆▅▆▄▅▅▆▆▆▅▄
wandb:            eval/META_TUNING/spider_with_cell.cfg/exact_match ▅▄▄▂▄▂▂▃▂▄▂▁▃▃▄▃▄▆▄▃▄▆▄▃▃▃▃▃▃▄▃▃▄▄▃▄▅▅▅█
wandb:                   eval/META_TUNING/spider_with_cell.cfg/exec ▆▅▇▅▅▄▃▄▃▃▂▁▂▃▅▄▃▄▄▃▃▅▄▄▃▄▅▃▄▅▄▄▄▄▅▅▆▆▆█
wandb:                                                     eval/avr ▆▅▅▄▅▃▂▄▃▄▂▁▃▃▅▄▄▅▄▃▃▅▄▃▃▄▄▃▄▅▄▄▄▄▄▄▅▆▅█
wandb:                                          train/train_runtime ▁
wandb:                               train/train_samples_per_second ▁
wandb:                                 train/train_steps_per_second ▁
wandb:                                             train/total_flos ▁
wandb:                                             train/train_loss ▁
wandb:                                           train/predict_loss ▁
wandb:                                        train/predict_runtime ▁
wandb:                             train/predict_samples_per_second ▁
wandb:   train/predict_META_TUNING/spider_with_cell.cfg/exact_match ▁
wandb:          train/predict_META_TUNING/spider_with_cell.cfg/exec ▁
wandb:                                            train/predict_avr ▁
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 66 other file(s)
```

```
zzy: setting training and evaluation steps both 5
export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246
export WANDB_PROJECT='text-to-sql(subset-spider-24Jan)'
export WANDB_ENTITY=leamonzea929
python -m torch.distributed.launch      --nproc_per_node 1      --master_port 1234 train.py      --seed 2      --cfg Salesforce/T5_base_prefix_spider_with_cell_value.cfg      --run_name T5_base_prefix_spider_with_cell_value      --logging_strategy steps      --logging_first_step true      --logging_steps 4      --evaluation_strategy steps      --eval_steps 5      --metric_for_best_model avr      --greater_is_better true      --save_strategy steps      --save_steps 5      --save_total_limit 1      --load_best_model_at_end      --gradient_accumulation_steps 8      --num_train_epochs 5      --adafactor true      --learning_rate 1e-4      --do_train      --do_eval      --do_predict      --predict_with_generate      --output_dir output/T5_base_prefix_spider_with_cell_value     --overwrite_output_dir   --per_device_train_batch_size 2      --per_device_eval_batch_size 8      --generation_num_beams 4     --generation_max_length 512      --input_max_length 512      --ddp_find_unused_parameters true  
```



- Date: Feburary, 2023

#### run model on text2cypher

```
export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246
export WANDB_PROJECT='text-to-sql(text2cyphr_Feb)'
export WANDB_ENTITY=leamonzea929
python -m torch.distributed.launch         --nproc_per_node 1          --master_port 1234 train.py          --seed 2          --cfg Salesforce/T5_base_prefix_text2cypher_with_cell_value.cfg          --run_name T5_base_prefix_text2cypher_with_cell_value         --logging_strategy steps          --logging_first_step true          --logging_steps 4          --evaluation_strategy steps          --eval_steps 5          --metric_for_best_model avr          --greater_is_better true          --save_strategy steps          --save_steps 5          --save_total_limit 1          --load_best_model_at_end          --gradient_accumulation_steps 8          --num_train_epochs 5          --adafactor true          --learning_rate 1e-4          --do_train          --do_eval          --do_predict          --predict_with_generate          --output_dir output/T5_base_prefix_text2cypher_with_cell_value      --overwrite_output_dir      --per_device_train_batch_size 2          --per_device_eval_batch_size 8          --generation_num_beams 2          --generation_max_length 64          --input_max_length 512          --ddp_find_unused_parameters true     

```

:point_right: run model on text2cypher on the following dataset setting.

```
train.json -> department_management
dev.json -> real_estate_properties + department_management

export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246
export WANDB_PROJECT='text2cypher-Feb(small-set)'
export WANDB_ENTITY=leamonzea929
python -m torch.distributed.launch              --nproc_per_node 1               --master_port 1234 train.py               --seed 2               --cfg Salesforce/T5_base_prefix_text2cypher_with_cell_value.cfg               --run_name T5_base_prefix_text2cypher_with_cell_value              --logging_strategy steps               --logging_first_step true               --logging_steps 500               --evaluation_strategy steps               --eval_steps 500               --metric_for_best_model avr               --greater_is_better true               --save_strategy steps               --save_steps 500               --save_total_limit 1               --load_best_model_at_end               --gradient_accumulation_steps 8               --num_train_epochs 500               --adafactor true               --learning_rate 1e-4               --do_train               --do_eval               --do_predict               --predict_with_generate               --output_dir output/T5_base_prefix_text2cypher_with_cell_value           --overwrite_output_dir           --per_device_train_batch_size 2               --per_device_eval_batch_size 8               --generation_num_beams 4               --generation_max_length 512               --input_max_length 512               --ddp_find_unused_parameters true 

***** predict metrics *****
  epoch                                                              =      500.0
  predict_META_TUNING/text2cypher_with_cell.cfg/exectuation accuracy =     0.3889
  predict_avr                                                        =     0.3889
  predict_loss                                                       =     0.6234
  predict_runtime                                                    = 0:00:17.61
  predict_samples                                                    =         18
  predict_samples_per_second                                         =      1.022

wandb: Waiting for W&B process to finish, PID 1124683
wandb: Program ended successfully.
wandb:                                                                                
wandb: Find user logs for this run at: /home/22842219/Desktop/phd/SemanticParser4Graph/semantic_parser/wandb/run-20230208_102148-2nt3tdbm/logs/debug.log
wandb: Find internal logs for this run at: /home/22842219/Desktop/phd/SemanticParser4Graph/semantic_parser/wandb/run-20230208_102148-2nt3tdbm/logs/debug-internal.log
wandb: Run summary:
wandb:                                                                 train/loss 0.8821
wandb:                                                        train/learning_rate 0.0
wandb:                                                                train/epoch 500.0
wandb:                                                          train/global_step 500
wandb:                                                                   _runtime 1659
wandb:                                                                 _timestamp 1675824567
wandb:                                                                      _step 5
wandb:                                                                  eval/loss 0.62338
wandb:                                                               eval/runtime 17.5908
wandb:                                                    eval/samples_per_second 1.023
wandb:            eval/META_TUNING/text2cypher_with_cell.cfg/exectuation accuracy 0.38889
wandb:                                                                   eval/avr 0.38889
wandb:                                                        train/train_runtime 1578.463
wandb:                                             train/train_samples_per_second 4.751
wandb:                                               train/train_steps_per_second 0.317
wandb:                                                           train/total_flos 5247970725330944.0
wandb:                                                           train/train_loss 0.89275
wandb:                                                         train/predict_loss 0.62338
wandb:                                                      train/predict_runtime 17.6191
wandb:                                           train/predict_samples_per_second 1.022
wandb:   train/predict_META_TUNING/text2cypher_with_cell.cfg/exectuation accuracy 0.38889
wandb:                                                          train/predict_avr 0.38889
wandb: Run history:
wandb:                                                                 train/loss █▁
wandb:                                                        train/learning_rate █▁
wandb:                                                                train/epoch ▁█████
wandb:                                                          train/global_step ▁█████
wandb:                                                                   _runtime ▁█████
wandb:                                                                 _timestamp ▁█████
wandb:                                                                      _step ▁▂▄▅▇█
wandb:                                                                  eval/loss ▁▁
wandb:                                                               eval/runtime █▁
wandb:                                                    eval/samples_per_second ▁█
wandb:            eval/META_TUNING/text2cypher_with_cell.cfg/exectuation accuracy ▁▁
wandb:                                                                   eval/avr ▁▁
wandb:                                                        train/train_runtime ▁
wandb:                                             train/train_samples_per_second ▁
wandb:                                               train/train_steps_per_second ▁
wandb:                                                           train/total_flos ▁
wandb:                                                           train/train_loss ▁
wandb:                                                         train/predict_loss ▁
wandb:                                                      train/predict_runtime ▁
wandb:                                           train/predict_samples_per_second ▁
wandb:   train/predict_META_TUNING/text2cypher_with_cell.cfg/exectuation accuracy ▁
wandb:                                                          train/predict_avr ▁
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
```


:point_right: run model on text2cypher on the following dataset setting.

:smiling_imp: local ubuntu machine experiments setting

```
export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246
export WANDB_PROJECT='text2cypher-Feb(whole-set)'
export WANDB_ENTITY=leamonzea929


python -m torch.distributed.launch              --nproc_per_node 1               --master_port 1234 train.py               --seed 2               --cfg Salesforce/T5_base_prefix_text2cypher_with_cell_value.cfg               --run_name T5_base_prefix_text2cypher_with_cell_value              --logging_strategy steps               --logging_first_step true               --logging_steps 500               --evaluation_strategy steps               --eval_steps 500               --metric_for_best_model avr               --greater_is_better true               --save_strategy steps               --save_steps 500               --save_total_limit 1               --load_best_model_at_end               --gradient_accumulation_steps 8               --num_train_epochs 500               --adafactor true               --learning_rate 5e-4               --do_train               --do_eval               --do_predict               --predict_with_generate               --output_dir output/T5_base_prefix_text2cypher_with_cell_value           --overwrite_output_dir           --per_device_train_batch_size 2               --per_device_eval_batch_size 8               --generation_num_beams 2              --generation_max_length 512               --input_max_length 512               --ddp_find_unused_parameters true 
```
:imp: java installation! 
```kaya experiments setting
export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246
export WANDB_PROJECT='text2cypher-Feb(whole-set)-kaya'
export WANDB_ENTITY=leamonzea929

#kaya
python -m torch.distributed.launch              --nproc_per_node 1               --master_port 1234 train.py               --seed 2               --cfg Salesforce/T5_base_prefix_text2cypher_with_cell_value.cfg               --run_name T5_base_prefix_text2cypher_with_cell_value              --logging_strategy steps               --logging_first_step true               --logging_steps 500               --evaluation_strategy steps               --eval_steps 500               --metric_for_best_model avr               --greater_is_better true               --save_strategy steps               --save_steps 500               --save_total_limit 1               --load_best_model_at_end               --gradient_accumulation_steps 8               --num_train_epochs 500               --adafactor true               --learning_rate 1e-4               --do_train               --do_eval               --do_predict               --predict_with_generate               --output_dir output/T5_base_prefix_text2cypher_with_cell_value           --overwrite_output_dir           --per_device_train_batch_size 2               --per_device_eval_batch_size 8               --generation_num_beams 4               --generation_max_length 512               --input_max_length 512               --ddp_find_unused_parameters true 
```