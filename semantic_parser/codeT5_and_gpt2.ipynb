{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schema that is serialized by the Cypher query.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base-multi-sum')\n",
    "\n",
    "    text = \"\"\" \"serialized_schema\": \" | concert_singer | :`concert_singer.stadium` : name , lowest , stadium_id , capacity , highest , location , average | :`concert_singer.singer` : country , age , name , song_name , is_male , singer_id , song_release_year | :`concert_singer.concert` : theme , stadium_id , concert_name , concert_id , year\",\n",
    "        \"struct_in\": \"| concert_singer | :`concert_singer.stadium` : name , lowest , stadium_id , capacity , highest , location , average | :`concert_singer.singer` : country , age , name , song_name , is_male , singer_id , song_release_year | :`concert_singer.concert` : theme , stadium_id , concert_name , concert_id , year\",\n",
    "        \"text_in\": \"How many singers do we have?, translate the natural language question to a Cypher query, given the serialized schema and structure input\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    generated_ids = model.generate(input_ids, max_length=512)\n",
    "    print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "    # this prints: \"Convert a SVG string to a QImage.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"| concert_singer | :`concert_singer.singer`, capacity, highest, location, average| :`concert_singer.singer` : country,,,, capacity,, location, average| :`concert_singer.singer` :,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "\n",
    "\n",
    "text = \"\"\" \"serialized_schema\": \" | concert_singer | :`concert_singer.stadium` : name , lowest , stadium_id , capacity , highest , location , average | :`concert_singer.singer` : country , age , name , song_name , is_male , singer_id , song_release_year | :`concert_singer.concert` : theme , stadium_id , concert_name , concert_id , year\",\n",
    "    \"struct_in\": \"| concert_singer | :`concert_singer.stadium` : name , lowest , stadium_id , capacity , highest , location , average | :`concert_singer.singer` : country , age , name , song_name , is_male , singer_id , song_release_year | :`concert_singer.concert` : theme , stadium_id , concert_name , concert_id , year\",\n",
    "    \"text_in\": \"How many singers do we have?, translate the natural language question to a Cypher query, given the serialized schema and structure input\"\"\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# simply generate one code span\n",
    "generated_ids = model.generate(input_ids, max_length=512)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "# this prints \"{user.username}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 153/153 [00:00<00:00, 167kB/s]\n",
      "Downloading: 100%|██████████| 610/610 [00:00<00:00, 521kB/s]\n",
      "Downloading: 100%|██████████| 959k/959k [00:01<00:00, 603kB/s]  \n",
      "Downloading: 100%|██████████| 518k/518k [00:01<00:00, 391kB/s]  \n",
      "Downloading: 100%|██████████| 358/358 [00:00<00:00, 276kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading: 100%|██████████| 516M/516M [00:29<00:00, 17.6MB/s]   \n"
     ]
    }
   ],
   "source": [
    "#codegpt-small-java\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/CodeGPT-small-java\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/CodeGPT-small-java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " public static List <\n"
     ]
    }
   ],
   "source": [
    "text = \"How many singers do we have?\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# simply generate one code span\n",
    "generated_ids = model.generate(input_ids, max_length=512)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "# this prints \"{user.username}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import openai\n",
    "import time\n",
    "import multiprocessing\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def get_min_val(pairs):\n",
    "    min_val = float(\"inf\")\n",
    "    for key, val in pairs:\n",
    "        if val < min_val:\n",
    "            min_key = key\n",
    "            min_val = val\n",
    "    return min_key, min_val\n",
    "\n",
    "def generate_line(line):\n",
    "    time_gap = 5\n",
    "    sleep_time = 5\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        '''\n",
    "        这一段sleep的逻辑是这样的:\n",
    "        1. 首先维护一个全局字典prev_times, 里面记录了每个key token上一次被调用的时间\n",
    "        2. 然后每一次调用generate_line时, 都会查找里面被调用时间最久的key token\n",
    "        3. 如果被调用时间最久的key token被调用的时间距当前小于time_gap, 那就让当前线程sleep一下\n",
    "        4. 如果当前被调用的key token产生了RateLimitError, 那也让当前线程sleep一下\n",
    "        '''\n",
    "\n",
    "        min_key, min_val = get_min_val(prev_times.items())\n",
    "        \n",
    "        while time.time() - min_val < time_gap:\n",
    "            time.sleep(random.randint(sleep_time, sleep_time+2))\n",
    "            print(\"Too fast! Have a rest!\")\n",
    "            min_key, min_val = get_min_val(prev_times.items())\n",
    "\n",
    "        openai.api_key = min_key\n",
    "\n",
    "        try:\n",
    "            prev_times[min_key] = time.time()\n",
    "            response = openai.Completion.create(\n",
    "                engine=\"text-davinci-003\",\n",
    "                prompt=line,\n",
    "                temperature=0.5,\n",
    "                max_tokens=1024,\n",
    "                stop=[\"\\n\\n\\n\"],\n",
    "            )\n",
    "        except openai.error.RateLimitError:\n",
    "            print(\"RateLimitError! Have a rest! The key is \" + min_key)\n",
    "            time.sleep(random.randint(sleep_time, sleep_time+2))\n",
    "        except openai.error.APIError:\n",
    "            print(\"API error! Have a rest!\")\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            # if \"<|im_end|>\" not in response['choices'][0]['text']:\n",
    "            #     print(\"Exceptional line without end token! Retrying!\")\n",
    "            #     print(\"The input is: \"+src_line)\n",
    "            #     continue\n",
    "            if response['choices'][0]['text'].strip() == \"\":\n",
    "                print(\"Exceptional line with empty output! Retrying!\")\n",
    "                # print(\"The input is: \"+line)\n",
    "                continue\n",
    "            break\n",
    "\n",
    "    counter.value += 1\n",
    "    print(f\"%d lines finished in %.2f seconds. %.2f seconds per line on average.\" % (counter.value, time.time()-start_time, (time.time()-start_time)/counter.value))\n",
    "\n",
    "    text = response['choices'][0]['text'].strip()\n",
    "    # print(text)\n",
    "    # if \"\\n\" in text:\n",
    "    #     print(\"Exceptional line with line breaker inside the senence:\")\n",
    "    #     print(text)\n",
    "    #     text = text.replace(\"\\n\", \" \")\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' | concert_singer | :`concert_singer.stadium` : name , lowest , stadium_id , capacity , highest , location , average | :`concert_singer.singer` : country , age , name , song_name , is_male , singer_id , song_release_year | :`concert_singer.concert` : theme , stadium_id , concert_name , concert_id , year SEP | concert_singer | :`concert_singer.stadium` : name , lowest , stadium_id , capacity , highest , location , average | :`concert_singer.singer` : country , age , name , song_name , is_male , singer_id , song_release_year | :`concert_singer.concert` : theme , stadium_id , concert_name , concert_id , year SEP How many singers do we have?']\n",
      "[' | concert_singer | :`concert_singer.stadium` : name , lowest , stadium_id , capacity , highest , location , average | :`concert_singer.singer` : country , age , name , song_name , is_male , singer_id , song_release_year | :`concert_singer.concert` : theme , stadium_id , concert_name , concert_id , year SEP | concert_singer | :`concert_singer.stadium` : name , lowest , stadium_id , capacity , highest , location , average | :`concert_singer.singer` : country , age , name , song_name , is_male , singer_id , song_release_year | :`concert_singer.concert` : theme , stadium_id , concert_name , concert_id , year SEP How many singers do we have?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:: 100%|██████████| 1/1 [00:00<00:00, 5256.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{content}\n",
      "\n",
      "Translate a natural language question to a Cypher query:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too fast! Have a rest!\n",
      "1 lines finished in 18.98 seconds. 18.98 seconds per line on average.\n",
      "heyyy: (' | concert_singer | :`concert_singer.stadium` : name , lowest , stadium_id , capacity , highest , location , average | :`concert_singer.singer` : country , age , name , song_name , is_male , singer_id , song_release_year | :`concert_singer.concert` : theme , stadium_id , concert_name , concert_id , year SEP | concert_singer | :`concert_singer.stadium` : name , lowest , stadium_id , capacity , highest , location , average | :`concert_singer.singer` : country , age , name , song_name , is_male , singer_id , song_release_year | :`concert_singer.concert` : theme , stadium_id , concert_name , concert_id , year SEP How many singers do we have?', '{content}\\n\\nTranslate a natural language question to a Cypher query:', 'Question: How many singers do we have?\\n\\nCypher Query: MATCH (s:Singer) RETURN COUNT(s)')\n",
      "One batch finished! Line counting: 1\n",
      "Finished! Totally 18.998821 seconds used.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file ='/home/22842219/Desktop/phd/SemanticParser4Graph/semantic_parser/gpt_input.json'\n",
    "output_file = '/home/22842219/Desktop/phd/SemanticParser4Graph/semantic_parser/gpt_output.json'\n",
    "tokens = ['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']\n",
    "pool_number_per_token=2\n",
    "\n",
    "prompts = [\"{content}\\n\\nTranslate a natural language question to a Cypher query:\"]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "prev_times = manager.dict()\n",
    "counter = manager.Value(\"counter\", 0)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "keys = tokens\n",
    "\n",
    "for key in keys:\n",
    "    prev_times[key] = time.time()\n",
    "\n",
    "CNT = 0\n",
    "        \n",
    "pool_number = pool_number_per_token * len(keys)\n",
    "with multiprocessing.Pool(pool_number) as pool,\\\n",
    "open(input_file, \"r\", encoding=\"utf-8\") as fsrc,\\\n",
    "open(output_file, \"a+\", encoding=\"utf-8\") as fout:\n",
    "    data = json.load(fsrc)\n",
    "    src_lines = [data['serialized_schema']+ ' SEP '+ data['struct_in']+ ' SEP ' + data['text_in']]\n",
    "\n",
    "    # src_lines = [line.strip() for line in fsrc.readlines()]\n",
    "    print(src_lines)\n",
    "\n",
    "    sub_times = (len(src_lines)-1) // 1000 + 1\n",
    "\n",
    "    for sub_id in range(sub_times):\n",
    "        if sub_id != sub_times-1:\n",
    "            sub_src_lines = src_lines[sub_id*1000:(sub_id+1)*1000]\n",
    "        else:\n",
    "            sub_src_lines = src_lines[sub_id*1000:]\n",
    "        print(sub_src_lines)\n",
    "\n",
    "        lines = []\n",
    "        lines_prompts = []\n",
    "        short_src_lines = []\n",
    "        for line in tqdm(sub_src_lines, desc=\"Tokenizing:\"):\n",
    "            # line = json.loads(line)\n",
    "            # Omit lines with a length smaller than 3072\n",
    "            # if len(tokenizer.tokenize(line[\"content\"])) < 3072:\n",
    "            prompt = random.choice(prompts)\n",
    "            print(prompt)\n",
    "            lines.append(prompt.format(content=line))\n",
    "            lines_prompts.append(prompt)\n",
    "            short_src_lines.append(line)\n",
    "\n",
    "            CNT += 1\n",
    "        \n",
    "        results = pool.map(generate_line, lines, pool_number)\n",
    "        for line in zip(short_src_lines, lines_prompts, results):\n",
    "            print(\"heyyy:\", line)\n",
    "            json_line = {}\n",
    "            json_line['input'] = line[0]\n",
    "            json_line[\"prompt\"] = line[1]\n",
    "            json_line[\"output\"] = line[2]\n",
    "            fout.write(json.dumps(json_line, ensure_ascii=False)+\"\\n\")\n",
    "        \n",
    "        print(f\"One batch finished! Line counting: {CNT}\")\n",
    "\n",
    "stop_time = time.time()  \n",
    "print(\"Finished! Totally %f seconds used.\" % (stop_time-start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
