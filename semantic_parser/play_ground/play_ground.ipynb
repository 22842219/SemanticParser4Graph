{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.9.2\n",
    "!pip install nltk\n",
    "!pip install datasets==1.14.0\n",
    "!pip install sentencepiece\n",
    "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play text2cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2cypher(text, model, tokenizer):\n",
    "    #model\n",
    "    print(\"=====â“Request=====\")\n",
    "    print(text)\n",
    "    tokenized_txt = tokenizer([text], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    pred = tokenizer.batch_decode(\n",
    "        model.generate(\n",
    "            torch.LongTensor(tokenized_txt.data['input_ids']),\n",
    "            torch.LongTensor(tokenized_txt.data['attention_mask']),\n",
    "            num_beams=1, \n",
    "            max_length=256\n",
    "            ), \n",
    "        skip_special_tokens=True \n",
    "    ) # More details see utils/dataset.py and utils/trainer.py\n",
    "    print(\"=====ðŸ’¡Answer=====\")\n",
    "    print(pred)\n",
    "    # text2cypher = 'MATCH (singer:`concert_singer.singer`) RETURN count(*)'\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file /home/22842219/Desktop/phd/SemanticParser4Graph/application/rel_db2kg/text2sql2cypher/pretrain/config.json not found\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for '/home/22842219/Desktop/phd/SemanticParser4Graph/application/rel_db2kg/text2sql2cypher/pretrain'. Make sure that:\n\n- '/home/22842219/Desktop/phd/SemanticParser4Graph/application/rel_db2kg/text2sql2cypher/pretrain' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/home/22842219/Desktop/phd/SemanticParser4Graph/application/rel_db2kg/text2sql2cypher/pretrain' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:512\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    511\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_path(\n\u001b[1;32m    513\u001b[0m         config_file,\n\u001b[1;32m    514\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    515\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    516\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    517\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    518\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    519\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    520\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[1;32m    522\u001b[0m     \u001b[39m# Load config dict\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/file_utils.py:1385\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[39melif\u001b[39;00m urlparse(url_or_filename)\u001b[39m.\u001b[39mscheme \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1384\u001b[0m     \u001b[39m# File, but it doesn't exist.\u001b[39;00m\n\u001b[0;32m-> 1385\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfile \u001b[39m\u001b[39m{\u001b[39;00murl_or_filename\u001b[39m}\u001b[39;00m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1387\u001b[0m     \u001b[39m# Something unknown\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: file /home/22842219/Desktop/phd/SemanticParser4Graph/application/rel_db2kg/text2sql2cypher/pretrain/config.json not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m         AutoTokenizer, \n\u001b[1;32m      3\u001b[0m         T5ForConditionalGeneration)\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mSalesforce/codet5-base\u001b[39m\u001b[39m\"\u001b[39m, use_fast\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m T5ForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39m/home/22842219/Desktop/phd/SemanticParser4Graph/application/rel_db2kg/text2sql2cypher/pretrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:1187\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   1186\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 1187\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   1188\u001b[0m         config_path,\n\u001b[1;32m   1189\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args,\n\u001b[1;32m   1190\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1191\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1192\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1193\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1194\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1195\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1196\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1197\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1198\u001b[0m         _from_auto\u001b[39m=\u001b[39;49mfrom_auto_class,\n\u001b[1;32m   1199\u001b[0m         _from_pipeline\u001b[39m=\u001b[39;49mfrom_pipeline,\n\u001b[1;32m   1200\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1201\u001b[0m     )\n\u001b[1;32m   1202\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:455\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    387\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m    Instantiate a :class:`~transformers.PretrainedConfig` (or a derived class) from a pretrained model\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39m    configuration.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \n\u001b[1;32m    454\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    456\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m    457\u001b[0m         logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    458\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    460\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:532\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     logger\u001b[39m.\u001b[39merror(err)\n\u001b[1;32m    527\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load config for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Make sure that:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m- \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is a correct model identifier listed on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m- or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory containing a \u001b[39m\u001b[39m{\u001b[39;00mCONFIG_NAME\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[0;32m--> 532\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(msg)\n\u001b[1;32m    534\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError:\n\u001b[1;32m    535\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    536\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt reach server at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mconfig_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to download configuration file or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconfiguration file is not a valid JSON file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease check network or file content here: \u001b[39m\u001b[39m{\u001b[39;00mresolved_config_file\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for '/home/22842219/Desktop/phd/SemanticParser4Graph/application/rel_db2kg/text2sql2cypher/pretrain'. Make sure that:\n\n- '/home/22842219/Desktop/phd/SemanticParser4Graph/application/rel_db2kg/text2sql2cypher/pretrain' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/home/22842219/Desktop/phd/SemanticParser4Graph/application/rel_db2kg/text2sql2cypher/pretrain' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "        AutoTokenizer, \n",
    "        T5ForConditionalGeneration)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\", use_fast=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/home/22842219/Desktop/phd/SemanticParser4Graph/application/rel_db2kg/text2sql2cypher/pretrain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====â“Request=====\n",
      "how many singers do we have?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate() got multiple values for argument 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[39m=\u001b[39m text2cypher(\u001b[39m'\u001b[39;49m\u001b[39mhow many singers do we have?\u001b[39;49m\u001b[39m'\u001b[39;49m, model, tokenizer)\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mtext2cypher\u001b[0;34m(text, model, tokenizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(text)\n\u001b[1;32m      5\u001b[0m tokenized_txt \u001b[39m=\u001b[39m tokenizer([text], max_length\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m pred \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(\n\u001b[0;32m----> 7\u001b[0m     model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      8\u001b[0m         torch\u001b[39m.\u001b[39;49mLongTensor(tokenized_txt\u001b[39m.\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]),\n\u001b[1;32m      9\u001b[0m         torch\u001b[39m.\u001b[39;49mLongTensor(tokenized_txt\u001b[39m.\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m]),\n\u001b[1;32m     10\u001b[0m         num_beams\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m     11\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m\n\u001b[1;32m     12\u001b[0m         ), \n\u001b[1;32m     13\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m \n\u001b[1;32m     14\u001b[0m ) \u001b[39m# More details see utils/dataset.py and utils/trainer.py\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=====ðŸ’¡Answer=====\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(pred)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: generate() got multiple values for argument 'max_length'"
     ]
    }
   ],
   "source": [
    "pred = text2cypher('how many singers do we have?', model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/UnifiedSKGG-CYPHER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from utils.configue import Configure\n",
    "from utils.training_arguments import WrappedSeq2SeqTrainingArguments\n",
    "\n",
    "from filelock import FileLock\n",
    "import nltk\n",
    "with FileLock(\".lock\") as lock:\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "    nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(txt, model, tokenizer):\n",
    "  print(\"=====â“Request=====\")\n",
    "  print(txt)\n",
    "  tokenized_txt = tokenizer([txt], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "  pred = tokenizer.batch_decode(\n",
    "      model.generate(\n",
    "        torch.LongTensor(tokenized_txt.data['input_ids']),\n",
    "        torch.LongTensor(tokenized_txt.data['attention_mask']),\n",
    "        num_beams=1, \n",
    "        max_length=256\n",
    "        ), \n",
    "      skip_special_tokens=True \n",
    "  ) # More details see utils/dataset.py and utils/trainer.py\n",
    "  print(\"=====ðŸ’¡Answer=====\")\n",
    "  print(pred)\n",
    "  return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Spider (with cell value)\n",
    "\n",
    "# Set args here for runnning on notebook, we make them out here to make it more illustrative.\n",
    "sys.argv = ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', # This is the name of your .py launcher when you run this line of code.\n",
    "            # belows are the parameters we set, take spider for example\n",
    "            '--cfg', 'Salesforce/T5_base_prefix_spider_with_cell_value.cfg', \n",
    "            '--output_dir', './tmp']\n",
    "parser = HfArgumentParser((WrappedSeq2SeqTrainingArguments,))\n",
    "training_args, = parser.parse_args_into_dataclasses()\n",
    "set_seed(training_args.seed)\n",
    "args = Configure.Get(training_args.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model(21->1 multitasked prefix)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hkunlp/from_all_T5_base_prefix_spider_with_cell_value2\", use_fast=False)\n",
    "from models.unified.prefixtuning import Model\n",
    "model = Model(args)\n",
    "model.load(\"hkunlp/from_all_T5_base_prefix_spider_with_cell_value2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_in = \"| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country ( France ) , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id\"\n",
    "text_in = \"what is the minimum, average, and maximum age of all singers from France?\"\n",
    "# seq_out = \"SELECT avg(age) ,  min(age) ,  max(age) FROM singer WHERE country  =  'France'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sql = play(\"{}; structed knowledge: {}\".format(text_in, struct_in), model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_txt = tokenizer([text_in], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "print(\"tokenized_txt:\", tokenized_txt)\n",
    "print(\"tokenized_txt:\", len(tokenized_txt['input_ids'][0]))\n",
    "print(\"tokenized_txt:\", len(tokenized_txt['attention_mask'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Ziyu Zhao\n",
    "Affiliation: UWA NLT-TLP GROUP\n",
    "'''\n",
    "\n",
    "import os, re, math\n",
    "from typing import Set\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "class DBengine:\n",
    "    \n",
    "    \"\"\"\n",
    "    An Entity, which takes a sqlite3 database schema.\n",
    "    Attributes:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fdb):\n",
    "\n",
    "        self.conn = sqlite3.connect(fdb)      \n",
    "        self.conn.text_factory = lambda b: b.decode(errors = 'ignore')\n",
    "\n",
    "    \n",
    "    def execute(self, query):\n",
    "      cursor = self.conn.cursor()  \n",
    "      return cursor.execute(query)  \n",
    "    \n",
    "    def get_table_names(self):\n",
    "        cursor = self.conn.cursor()  \n",
    "        return cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")     \n",
    "    \n",
    "    def get_table_values(self, table_name):\n",
    "        cursor = self.conn.cursor()  \n",
    "        return cursor.execute('SELECT * from {}'.format(table_name))  \n",
    "\n",
    "    def get_schema(self, table_name):\n",
    "        cursor = self.conn.cursor()  \n",
    "        cursor.execute('SELECT sql FROM sqlite_master WHERE type = \"table\" and name= \"{}\";'.format(table_name))  \n",
    "        result = cursor.fetchone()\n",
    "        if result is None:\n",
    "            raise ValueError(\"Table %s does not exist\" % table_name)\n",
    "        return result[0].strip()  \n",
    "\n",
    "    def get_outbound_foreign_keys(self, table_name):\n",
    "        cursor = self.conn.cursor() \n",
    "        infos = cursor.execute(\"PRAGMA foreign_key_list([{}])\".format(table_name)).fetchall()\n",
    "        table_constraints = []\n",
    "        pks_fks_dict ={}\n",
    "        for info in infos:\n",
    "            if info is not None:\n",
    "                id, seq, ref_table, from_, to_, on_update, on_delete, match = info\n",
    "                table_constraints.append(\n",
    "                {\"this_table\": table_name, \"column\": from_, \"ref_table\": ref_table, \"ref_column\": to_}\n",
    "                )\n",
    "                pks_fks_dict[from_]= to_  # ref_column (aka pk in ref_table) is supposed to be the same with column. \n",
    "        return table_constraints, pks_fks_dict\n",
    "\n",
    "    def get_primay_keys(self, table_name):\n",
    "        cursor = self.conn.cursor() \n",
    "        pks = cursor.execute('SELECT l.name FROM pragma_table_info(\"{}\") as l WHERE l.pk <> 0;'.format(table_name)).fetchall()\n",
    "        return pks  \n",
    "    \n",
    "    def check_compound_pk(self, primary_keys):\n",
    "        compound_pk_check=False\n",
    "        if len(primary_keys)>1: \n",
    "            compound_pk_check=True\n",
    "        return compound_pk_check\n",
    "\n",
    "    def close(self):\n",
    "        self.conn.cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '/home/22842219/Desktop/openSource/UnifiedSKGG-Cypher/data/downloads/spider/database/concert_singer/concert_singer.sqlite'\n",
    "engine = DBengine(db_path)\n",
    "for sql_query in generated_sql:\n",
    "    print(sql_query)\n",
    "    sql_ans = engine.execute(sql_query).fetchall()\n",
    "    print(\"sql_ans:\", sql_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py2neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from py2neo import Graph\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/home/22842219/Desktop/phd/SemanticParser4Graph/application/config.ini')\n",
    "filenames = config[\"FILENAMES\"]\n",
    "\n",
    "neo4j_uri = filenames['neo4j_uri']\n",
    "neo4j_user = filenames['neo4j_user']\n",
    "neo4j_password = filenames['neo4j_password']\n",
    "graph = Graph(neo4j_uri, auth = (neo4j_user, neo4j_password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql2cypher = \"MATCH (singer:`concert_singer.singer`) \\\n",
    "WHERE singer.Country = 'France' \\\n",
    "RETURN min(singer.Age),avg(singer.Age),max(singer.Age)\"\n",
    "cypher_res = graph.run(sql2cypher).data()\n",
    "print(cypher_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end-to-end translation from text-to-cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## unifiedSKG learning process*******************\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from utils.configue import Configure\n",
    "from utils.training_arguments import WrappedSeq2SeqTrainingArguments\n",
    "\n",
    "from collections import OrderedDict\n",
    "import utils.tool\n",
    "from utils.dataset import TokenizedDataset\n",
    "from utils.trainer import EvaluateFriendlySeq2SeqTrainer\n",
    "\n",
    "# Huggingface realized the \"Seq2seqTrainingArguments\" which is the same with \"WrappedSeq2SeqTrainingArguments\"\n",
    "# in transformers==4.10.1 during our work.\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Configure.Get(training_args.cfg)\n",
    "print(training_args.cfg)\n",
    "print(args.bert, args.bert.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'checkpoint-???' in args.bert.location:\n",
    "    args.bert.location = get_last_checkpoint(\n",
    "        os.path.dirname(args.bert.location.model_name_or_path))\n",
    "    logger.info(f\"Resolve model_name_or_path to {args.bert.location.model_name_or_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = None\n",
    "if os.path.isdir(\n",
    "        training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "\n",
    "os.makedirs(training_args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(not args.arg_paths)\n",
    "print(args.dataset.loader_path)  ## hey, attention!\n",
    "print(args.dataset.data_store_path)\n",
    "print(args.seq2seq.constructor)\n",
    "print(args.arg_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs will be train, dev, test or train, dev now.\n",
    "# We deprecate the k-fold cross-valid function \n",
    "# since it causes too many avoidable troubles.\n",
    "if not args.arg_paths:\n",
    "    cache_root = os.path.join('output', 'cache')\n",
    "    os.makedirs(cache_root, exist_ok=True)\n",
    "    raw_datasets_split: datasets.DatasetDict = datasets.load_dataset(path=args.dataset.loader_path,\n",
    "                                                                        cache_dir=args.dataset.data_store_path)\n",
    "    seq2seq_dataset_split: tuple = utils.tool.get_constructor(args.seq2seq.constructor)(args).to_seq2seq(\n",
    "        raw_datasets_split, cache_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_root = os.path.join('output', 'cache')\n",
    "print(f'cache_root: {cache_root}')\n",
    "os.makedirs(cache_root, exist_ok=True)\n",
    "meta_tuning_data = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task, arg_path in args.arg_paths:\n",
    "    task_args = Configure.Get(arg_path)\n",
    "    task_args.bert = args.bert\n",
    "    print(f'task: {task}, arg_path: {arg_path}, task_args: {task_args}')\n",
    "    print('task_args.bert.location:', task_args.bert.location)\n",
    "    print(f'task_args.dataset: {task_args.dataset}, task_args.dataset.loader_path: {task_args.dataset.loader_path}')\n",
    "    print(f'task_args.dataset.data_store_path: {task_args.dataset.data_store_path}')\n",
    "    task_raw_datasets_split: datasets.DatasetDict = datasets.load_dataset(\n",
    "        path=task_args.dataset.loader_path,\n",
    "        cache_dir=task_args.dataset.data_store_path)\n",
    "    print(f'task_raw_datasets_split: {task_raw_datasets_split}')\n",
    "\n",
    "    print(f'task_args.seq2seq.constructor: {task_args.seq2seq.constructor}')\n",
    "    task_seq2seq_dataset_split: tuple = utils.tool.get_constructor(task_args.seq2seq.constructor)(task_args).\\\n",
    "        to_seq2seq(task_raw_datasets_split, cache_root)\n",
    "\n",
    "    meta_tuning_data[arg_path] = task_seq2seq_dataset_split\n",
    "    print(f'arg_path: {arg_path}')\n",
    "    print(f'meta_tuning_data: {meta_tuning_data}')\n",
    "\n",
    "seq2seq_dataset_split: tuple = utils.tool.get_constructor(args.seq2seq.constructor)(args).\\\n",
    "    to_seq2seq(meta_tuning_data)\n",
    "print(f'seq2seq_dataset_split: {seq2seq_dataset_split}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(task_args)\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'args: {args}, args.evaluate.tool: {args.evaluate.tool}')\n",
    "evaluator = utils.tool.get_evaluator(args.evaluate.tool)(args)\n",
    "print(f'evaluator: {evaluator}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'args.model.name: {args.model.name}, args: {args}')\n",
    "model = utils.tool.get_model(args.model.name)(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tokenizer = model.tokenizer\n",
    "model_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_train_dataset, seq2seq_eval_dataset, seq2seq_test_dataset = None, None, None\n",
    "print(f'seq2seq_train_dataset: {seq2seq_train_dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len(seq2seq_dataset_split): {len(seq2seq_dataset_split)}')\n",
    "if len(seq2seq_dataset_split) == 2:\n",
    "    seq2seq_train_dataset, seq2seq_eval_dataset = seq2seq_dataset_split\n",
    "elif len(seq2seq_dataset_split) == 3:\n",
    "    seq2seq_train_dataset, seq2seq_eval_dataset, seq2seq_test_dataset = seq2seq_dataset_split\n",
    "    print(f'seq2seq_train_dataset: {seq2seq_train_dataset}, seq2seq_eval_dataset: {seq2seq_eval_dataset} \\\n",
    "        seq2seq_test_dataset: {seq2seq_test_dataset}')\n",
    "else:\n",
    "    raise ValueError(\"Other split not support yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import TokenizedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wrap the \"string\" seq2seq data into \"tokenized tensor\".\n",
    "train_dataset = TokenizedDataset(args, training_args, model_tokenizer,\n",
    "                                    seq2seq_train_dataset) if seq2seq_train_dataset else None\n",
    "eval_dataset = TokenizedDataset(args, training_args, model_tokenizer,\n",
    "                                seq2seq_eval_dataset) if seq2seq_eval_dataset else None\n",
    "test_dataset = TokenizedDataset(args, training_args, model_tokenizer,\n",
    "                                seq2seq_test_dataset) if seq2seq_test_dataset else None\n",
    "print(f'train_dataset: {train_dataset}')\n",
    "print(f'eval_dataset: {eval_dataset}')\n",
    "print(f'test_dataset: {test_dataset}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=args.seq2seq.patience if args.seq2seq.patience else 5)\n",
    "print(f'early_stopping_callback: {early_stopping_callback}')\n",
    "print(f'patience: {args.seq2seq.patience}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.trainer import EvaluateFriendlySeq2SeqTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'seq2seq_eval_dataset: {seq2seq_eval_dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export WANDB_API_KEY=edd277e1d782e96634109705a6235577032da246\n",
    "!export WANDB_PROJECT='text-to-sql(text2cypher-24Jan)'\n",
    "!export WANDB_ENTITY=leamonzea929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_args.report_to)\n",
    "print(training_args.local_rank <=0)\n",
    "print(\"MLFLOW_EXPERIMENT_ID\" in os.environ)\n",
    "print(os.getenv(\"WANDB_PROJECT\", \"uni-frame-for-knowledge-tabular-tasks\"))\n",
    "print(training_args.run_name)\n",
    "print(os.getenv(\"WANDB_ENTITY\", 'sgtnew'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = EvaluateFriendlySeq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    evaluator=evaluator,\n",
    "    # We name it \"evaluator\" while the hugging face call it \"Metric\",\n",
    "    # they are all f(predictions: List, references: List of dict) = eval_result: dict\n",
    "    tokenizer=model_tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    eval_examples=seq2seq_eval_dataset,\n",
    "    # wandb_run_dir=wandb.run.dir if \"wandb\" in training_args.report_to and training_args.local_rank <= 0 else None,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "print('Trainer build successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_args.load_weights_from)\n",
    "\n",
    "# Load model weights (for --do_train=False or post finetuning).\n",
    "if training_args.load_weights_from:\n",
    "    state_dict = torch.load(os.path.join(training_args.load_weights_from, transformers.WEIGHTS_NAME), map_location=\"cpu\")\n",
    "    trainer.model.load_state_dict(state_dict, strict=True)\n",
    "    # release memory\n",
    "    del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.load_multiple_prefix_module_weights_from)\n",
    "if args.load_multiple_prefix_module_weights_from:\n",
    "    reconstruct_state_dict = OrderedDict()\n",
    "\n",
    "    # load prefix modules\n",
    "    for task_name, module_weight_location in args.load_multiple_prefix_module_weights_from:\n",
    "        state_dict = torch.load(os.path.join(module_weight_location, transformers.WEIGHTS_NAME), map_location=\"cpu\")\n",
    "        MULTI_PREFIX_ATTR_NAME = \"multi_prefix\"\n",
    "        for weight_name, stored_tensor in state_dict.items():\n",
    "            if str(weight_name).startswith(\"pretrain_model\"):\n",
    "                continue  # skip the pretrained model and we will load a new one from another place\n",
    "            reconstruct_state_dict['{}.{}.{}'.format(MULTI_PREFIX_ATTR_NAME, \"_\".join(task_name.split(\"_\")[:-1]), weight_name)] = stored_tensor\n",
    "            # extract the prefix part and add them to dict\n",
    "\n",
    "    # give it into the model\n",
    "    trainer.model.load_state_dict(reconstruct_state_dict, strict=False)\n",
    "\n",
    "    # release memory\n",
    "    del reconstruct_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
